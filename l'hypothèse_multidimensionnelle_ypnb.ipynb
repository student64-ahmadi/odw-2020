{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3/0HXnpgwlZXqLvGPx9ET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/student64-ahmadi/odw-2020/blob/master/l'hypoth%C3%A8se_multidimensionnelle_ypnb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdBSkTwkaUOT",
        "outputId": "de60d9b9-ac2a-4796-eb1d-ceb670a19fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgzrfiyAadFz",
        "outputId": "2291a8d9-811b-4550-c6bb-9cb151271815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuvuywo2axVc",
        "outputId": "b717f78f-59ec-498f-9bc5-673c05c6add9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqbGYY0Ub1Ui",
        "outputId": "b8924649-d33f-40ef-ca58-ba72860f159c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "pigdcuySZttk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gwpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqMuhIbhdkEf",
        "outputId": "05b4bb62-2566-45f6-a755-2e9cf2f5facc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gwpy in /usr/local/lib/python3.11/dist-packages (3.0.8)\n",
            "Requirement already satisfied: astropy>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gwpy) (7.0.1)\n",
            "Requirement already satisfied: dateparser>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from gwpy) (1.2.1)\n",
            "Requirement already satisfied: dqsegdb2 in /usr/local/lib/python3.11/dist-packages (from gwpy) (1.3.0)\n",
            "Requirement already satisfied: gwdatafind>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gwpy) (1.2.0)\n",
            "Requirement already satisfied: gwosc>=0.5.3 in /usr/local/lib/python3.11/dist-packages (from gwpy) (0.8.0)\n",
            "Requirement already satisfied: h5py>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from gwpy) (3.13.0)\n",
            "Requirement already satisfied: ligo-segments>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gwpy) (1.4.0)\n",
            "Requirement already satisfied: ligotimegps>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from gwpy) (2.0.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from gwpy) (3.7.3)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from gwpy) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from gwpy) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gwpy) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from gwpy) (1.14.1)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from gwpy) (4.67.1)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.11/dist-packages (from astropy>=4.3.0->gwpy) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.1.31.12.41.4 in /usr/local/lib/python3.11/dist-packages (from astropy>=4.3.0->gwpy) (0.2025.3.17.0.34.53)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy>=4.3.0->gwpy) (6.0.2)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy>=4.3.0->gwpy) (24.2)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.4->gwpy) (2025.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,>=2015.06.24 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.4->gwpy) (2024.11.6)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser>=1.1.4->gwpy) (5.3.1)\n",
            "Requirement already satisfied: igwn-auth-utils>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from gwdatafind>=1.1.0->gwpy) (1.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from ligo-segments>=1.0.0->gwpy) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->gwpy) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->gwpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->gwpy) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->gwpy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->gwpy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->gwpy) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gwpy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gwpy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gwpy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gwpy) (2025.1.31)\n",
            "Requirement already satisfied: click>=6.7 in /usr/local/lib/python3.11/dist-packages (from dqsegdb2->gwpy) (8.1.8)\n",
            "Requirement already satisfied: igwn-segments>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dqsegdb2->gwpy) (2.0.0)\n",
            "Requirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.11/dist-packages (from igwn-auth-utils>=0.3.1->gwdatafind>=1.1.0->gwpy) (43.0.3)\n",
            "Requirement already satisfied: safe-netrc>=1.0 in /usr/local/lib/python3.11/dist-packages (from igwn-auth-utils>=0.3.1->gwdatafind>=1.1.0->gwpy) (1.0.1)\n",
            "Requirement already satisfied: scitokens>=1.8 in /usr/local/lib/python3.11/dist-packages (from igwn-auth-utils>=0.3.1->gwdatafind>=1.1.0->gwpy) (1.8.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.2->igwn-auth-utils>=0.3.1->gwdatafind>=1.1.0->gwpy) (1.17.1)\n",
            "Requirement already satisfied: PyJWT>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from scitokens>=1.8->igwn-auth-utils>=0.3.1->gwdatafind>=1.1.0->gwpy) (2.10.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.2->igwn-auth-utils>=0.3.1->gwdatafind>=1.1.0->gwpy) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q 'gwpy==3.0.8'"
      ],
      "metadata": {
        "id": "jqOxvLv02lMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gwosc.datasets import event_gps\n",
        "gps = event_gps('GW170817')\n",
        "print(gps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSuifwjV3QS2",
        "outputId": "6faeefe6-f06e-472c-ad89-53edf7b60e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1187008882.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment = (int(gps)-5, int(gps)+5)\n",
        "print(segment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwZijCVf3cbJ",
        "outputId": "694c150a-839c-4feb-af1c-d6592bd338ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1187008877, 1187008887)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gwpy.timeseries import TimeSeries\n",
        "hdata = TimeSeries.fetch_open_data('H1', *segment, verbose=True)\n",
        "print(hdata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vji9_kOu3m9C",
        "outputId": "ce1dd1b9-d964-4e09-bebd-f01246716a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 1 URLs from gwosc.org for [1187008877 .. 1187008887))\n",
            "Reading data... [Done]\n",
            "TimeSeries([-4.70701276e-19, -4.77519734e-19, -4.70838351e-19,\n",
            "            ..., -3.65035985e-19, -3.80810766e-19,\n",
            "            -3.83224728e-19]\n",
            "           unit: dimensionless,\n",
            "           t0: 1187008877.0 s,\n",
            "           dt: 0.000244140625 s,\n",
            "           name: Strain,\n",
            "           channel: None)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "from gwpy.timeseries import TimeSeries\n",
        "\n",
        "# Données fournies (TimeSeries)\n",
        "# Convert the list to a numpy array to avoid copy issues\n",
        "strain_data = TimeSeries(\n",
        "    np.array([-4.70701276e-19, -4.77519734e-19, -4.70838351e-19, ..., -3.65035985e-19, -3.80810766e-19, -3.83224728e-19]), # Changed to np.array\n",
        "    unit='dimensionless',\n",
        "    t0=1187008877.0,\n",
        "    dt=0.000244140625,\n",
        "    name='Strain',\n",
        "    channel=None\n",
        ")\n",
        "\n",
        "# ... rest of the code remains the same ..."
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcmHQKHA5jvK",
        "outputId": "c7070ff5-4877-4554-846e-2ddece3de8e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: UnitsWarning: dimensionless is not a valid unit. Mathematical operations using this unit should work, but conversions to other units will not. [gwpy.detector.units]\n",
            "WARNING:astropy:UnitsWarning: dimensionless is not a valid unit. Mathematical operations using this unit should work, but conversions to other units will not.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib==3.7.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j47LPUVCLhy",
        "outputId": "18e20ff8-c104-4da2-b0ff-6b75d3eb1d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib==3.7.3 in /usr/local/lib/python3.11/dist-packages (3.7.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (1.4.8)\n",
            "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.3) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "from gwpy.timeseries import TimeSeries\n",
        "\n",
        "# Paramètres des données\n",
        "n_samples = 40960  # 10 secondes à 4096 Hz\n",
        "sampling_rate = 4096  # Hz\n",
        "dt = 1 / sampling_rate  # Intervalle de temps entre échantillons\n",
        "\n",
        "# Simuler des données de strain (car la liste fournie est incomplète)\n",
        "# Remplacez cette ligne par vos données réelles si disponibles\n",
        "np.random.seed(42)  # Pour la reproductibilité\n",
        "strain_values = np.random.normal(loc=-4e-19, scale=2e-20, size=n_samples)  # Bruit centré autour de -4e-19\n",
        "\n",
        "# Création de l’objet TimeSeries\n",
        "strain_data = TimeSeries(\n",
        "    strain_values,\n",
        "    unit=None,\n",
        "    t0=1187008877.0,\n",
        "    dt=dt,\n",
        "    name='Strain',\n",
        "    channel=None\n",
        ")\n",
        "\n",
        "# Paramètres pour l’analyse\n",
        "freq_min, freq_max = 10, 2000  # Plage de fréquences analysées (Hz), limitée par la fréquence de Nyquist (2048 Hz)\n",
        "\n",
        "# Nettoyage des données : suppression des valeurs aberrantes\n",
        "def clean_data(data, threshold=5):\n",
        "    median = np.median(data)\n",
        "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
        "    lower_bound = median - threshold * iqr\n",
        "    upper_bound = median + threshold * iqr\n",
        "    cleaned_data = np.where((data < lower_bound) | (data > upper_bound), median, data)\n",
        "    print(f\"Nombre de valeurs aberrantes remplacées : {np.sum((data < lower_bound) | (data > upper_bound))}\")\n",
        "    return cleaned_data\n",
        "\n",
        "# Modèle de bruit classique pour le strain : S(f) = A f^(-2) + B f^2 + C\n",
        "def noise_model(f, A, B, C):\n",
        "    return A * f**(-2) + B * f**2 + C\n",
        "\n",
        "# Modèle alternatif pour détecter des anomalies : S(f) = A f^C + B\n",
        "def anomaly_model(f, A, B, C):\n",
        "    return A * f**C + B\n",
        "\n",
        "# Création du rapport PDF\n",
        "with PdfPages(\"rapport_analyse_bruit_gwosc_1187008877.pdf\") as pdf:\n",
        "    report_text = []\n",
        "\n",
        "    # Extraction des données\n",
        "    data = strain_data.value\n",
        "    print(f\"Données chargées : {data.size} échantillons\")\n",
        "\n",
        "    # Nettoyage des NaN et des valeurs aberrantes\n",
        "    valid_data = data[~np.isnan(data)]\n",
        "    cleaned_data = clean_data(valid_data)\n",
        "\n",
        "    # Calcul de la densité spectrale de puissance (PSD) avec Welch\n",
        "    frequencies, S_f = welch(cleaned_data, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "    # Filtrer les fréquences pertinentes (10-2000 Hz)\n",
        "    mask = (frequencies >= freq_min) & (frequencies <= freq_max)\n",
        "    frequencies = frequencies[mask]\n",
        "    S_f = S_f[mask]\n",
        "\n",
        "    # Normalisation de S(f) par la médiane\n",
        "    S_f_norm = S_f / np.median(S_f)\n",
        "    print(f\"Valeur de normalisation (médiane) : {np.median(S_f)}\")\n",
        "    print(f\"Valeur de normalisation (maximum) : {np.max(S_f)}\")\n",
        "\n",
        "    # Vérification des données avant le tracé\n",
        "    if np.any(frequencies <= 0) or np.any(S_f_norm <= 0):\n",
        "        raise ValueError(\"Les fréquences ou S_f_norm contiennent des valeurs non positives, ce qui est invalide pour un tracé logarithmique.\")\n",
        "    print(f\"Min fréquence : {np.min(frequencies)}, Max fréquence : {np.max(frequencies)}\")\n",
        "    print(f\"Min S_f_norm : {np.min(S_f_norm)}, Max S_f_norm : {np.max(S_f_norm)}\")\n",
        "\n",
        "    # Ajustement du modèle classique\n",
        "    p0 = [1e-40, 1e-48, 1e-45]  # Valeurs initiales pour [A, B, C]\n",
        "    bounds = ([0, 0, 0], [np.inf, np.inf, np.inf])\n",
        "    try:\n",
        "        popt, pcov = curve_fit(noise_model, frequencies, S_f_norm, p0=p0, bounds=bounds, maxfev=10000)\n",
        "        A_fit, B_fit, C_fit = popt\n",
        "        report_text.append(f\"Modèle classique ajusté : S(f) = {A_fit:.2e} * f^(-2) + {B_fit:.2e} * f^2 + {C_fit:.2e}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Erreur dans l’ajustement classique : {e}\")\n",
        "        A_fit, B_fit, C_fit = 1e-40, 0, 1e-45\n",
        "        report_text.append(f\"Échec de l’ajustement classique, valeurs par défaut : S(f) = {A_fit:.2e} * f^(-2) + {B_fit:.2e} * f^2 + {C_fit:.2e}\")\n",
        "\n",
        "    # Calcul du spectre ajusté (modèle classique)\n",
        "    S_f_background = noise_model(frequencies, A_fit, B_fit, C_fit)\n",
        "\n",
        "    # Ajustement du modèle alternatif (pour détecter des anomalies)\n",
        "    p0_alt = [1e-5, 1e-5, -1.0]  # Valeurs initiales ajustées\n",
        "    bounds_alt = ([0, 0, -5], [np.inf, np.inf, 5])\n",
        "    try:\n",
        "        popt_alt, pcov_alt = curve_fit(anomaly_model, frequencies, S_f_norm, p0=p0_alt, bounds=bounds_alt, maxfev=10000)\n",
        "        A_alt, B_alt, C_alt = popt_alt\n",
        "        report_text.append(f\"Modèle alternatif ajusté : S(f) = {A_alt:.2e} * f^{C_alt:.2f} + {B_alt:.2e}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Erreur dans l’ajustement alternatif : {e}\")\n",
        "        A_alt, B_alt, C_alt = 1e-5, 1e-5, 1.0\n",
        "        report_text.append(f\"Échec de l’ajustement alternatif, valeurs par défaut : S(f) = {A_alt:.2e} * f^{C_alt:.2f} + {B_alt:.2e}\")\n",
        "\n",
        "    # Calcul du spectre ajusté (modèle alternatif)\n",
        "    S_f_anomaly = anomaly_model(frequencies, A_alt, B_alt, C_alt)\n",
        "\n",
        "    # Bootstrap pour estimer les incertitudes\n",
        "    n_bootstrap = 500\n",
        "    bootstrap_psds = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        sample = np.random.choice(cleaned_data, size=len(cleaned_data), replace=True)\n",
        "        _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "        psd_sample = psd_sample[mask] / np.median(psd_sample[mask])\n",
        "        bootstrap_psds.append(psd_sample)\n",
        "    bootstrap_psds = np.array(bootstrap_psds)\n",
        "    psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "    psd_std = np.std(bootstrap_psds, axis=0)\n",
        "    psd_std = np.where(psd_std == 0, 1e-10, psd_std)\n",
        "\n",
        "    # Calcul du chi2 pour le modèle classique\n",
        "    chi2_classic = np.sum(((S_f_norm - S_f_background) / psd_std) ** 2)\n",
        "    report_text.append(f\"Chi2 modèle classique : {chi2_classic:.2e}\")\n",
        "\n",
        "    # Calcul du chi2 pour le modèle alternatif\n",
        "    chi2_anomaly = np.sum(((S_f_norm - S_f_anomaly) / psd_std) ** 2)\n",
        "    report_text.append(f\"Chi2 modèle alternatif : {chi2_anomaly:.2e}\")\n",
        "\n",
        "    # Simulation Monte Carlo pour la p-valeur (modèle classique)\n",
        "    n_simulations = 100\n",
        "    chi2_simulated = []\n",
        "    for _ in range(n_simulations):\n",
        "        noise = np.random.normal(0, 1, size=len(cleaned_data)) * np.sqrt(np.mean(S_f))\n",
        "        _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "        psd_sim = psd_sim[mask] / np.median(psd_sim[mask])\n",
        "        chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "        chi2_simulated.append(chi2_val)\n",
        "    p_value_classic = np.mean(np.array(chi2_simulated) > chi2_classic)\n",
        "    report_text.append(f\"P-valeur modèle classique : {p_value_classic:.3f}\")\n",
        "\n",
        "    # Graphique 1 : Spectre observé vs ajusté (approche explicite)\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.loglog(frequencies, S_f_norm, label=\"S(f) observé (normalisé)\")\n",
        "    ax.loglog(frequencies, S_f_background, label=\"S(f) classique ajusté\", linestyle=\"--\")\n",
        "    ax.loglog(frequencies, S_f_anomaly, label=\"S(f) alternatif ajusté\", linestyle=\":\")\n",
        "    ax.fill_between(frequencies, psd_mean - 2*psd_std, psd_mean + 2*psd_std, alpha=0.2, label=\"Intervalle 95% (Bootstrap)\")\n",
        "    ax.set_xlabel(\"Fréquence (Hz)\")\n",
        "    ax.set_ylabel(\"Densité spectrale S(f) (normalisée)\")\n",
        "    ax.set_title(\"Spectre de bruit (GWOSC, 10s, 1187008877)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, which=\"both\", ls=\"--\")\n",
        "    pdf.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Graphique 2 : Série temporelle (pour rechercher un signal d’onde gravitationnelle)\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.plot(strain_data.times.value, strain_data.value, label=\"Strain\")\n",
        "    ax.set_xlabel(\"Temps (s)\")\n",
        "    ax.set_ylabel(\"Strain\")\n",
        "    ax.set_title(\"Série temporelle (GWOSC, 10s, 1187008877)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    pdf.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Page texte du rapport\n",
        "    fig, ax = plt.subplots(figsize=(8, 11))\n",
        "    ax.text(0.1, 0.95, \"Rapport d’analyse (GWOSC, 1187008877)\", fontsize=16, fontweight='bold')\n",
        "    ax.text(0.1, 0.9, \"\\n\".join(report_text), fontsize=12, verticalalignment='top')\n",
        "    ax.axis('off')\n",
        "    pdf.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Affichage console\n",
        "    print(\"\\n=== Rapport d’analyse ===\")\n",
        "    for line in report_text:\n",
        "        print(line)\n",
        "    print(\"=== Fin du Rapport ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avcjZvbqG1pE",
        "outputId": "1acb0364-450a-41da-a090-896e5be2b6f6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Données chargées : 40960 échantillons\n",
            "Nombre de valeurs aberrantes remplacées : 0\n",
            "Valeur de normalisation (médiane) : 1.9447821116294995e-43\n",
            "Valeur de normalisation (maximum) : 2.6745767256730534e-43\n",
            "Min fréquence : 12.0, Max fréquence : 2000.0\n",
            "Min S_f_norm : 0.6733420190935852, Max S_f_norm : 1.3752577780716377\n",
            "\n",
            "=== Rapport d’analyse ===\n",
            "Modèle classique ajusté : S(f) = 2.78e-14 * f^(-2) + 1.73e-09 * f^2 + 1.00e+00\n",
            "Modèle alternatif ajusté : S(f) = 1.08e-12 * f^-2.94 + 1.00e+00\n",
            "Chi2 modèle classique : 4.96e+02\n",
            "Chi2 modèle alternatif : 4.96e+02\n",
            "P-valeur modèle classique : 0.590\n",
            "=== Fin du Rapport ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "from gwpy.timeseries import TimeSeries\n",
        "\n",
        "# Paramètres des données\n",
        "n_samples = 40960  # 10 secondes à 4096 Hz\n",
        "sampling_rate = 4096  # Hz\n",
        "dt = 1 / sampling_rate  # Intervalle de temps entre échantillons\n",
        "\n",
        "# Simuler des données de strain (car la liste fournie est incomplète)\n",
        "# Remplacez cette ligne par vos données réelles si disponibles\n",
        "np.random.seed(42)  # Pour la reproductibilité\n",
        "strain_values = np.random.normal(loc=-4e-19, scale=2e-20, size=n_samples)  # Bruit centré autour de -4e-19\n",
        "\n",
        "# Création de l’objet TimeSeries\n",
        "strain_data = TimeSeries(\n",
        "    strain_values,\n",
        "    unit=None,\n",
        "    t0=1187008877.0,\n",
        "    dt=dt,\n",
        "    name='Strain',\n",
        "    channel=None\n",
        ")\n",
        "\n",
        "# Paramètres pour l’analyse\n",
        "freq_min, freq_max = 10, 2000  # Plage de fréquences analysées (Hz), limitée par la fréquence de Nyquist (2048 Hz)\n",
        "\n",
        "# Nettoyage des données : suppression des valeurs aberrantes\n",
        "def clean_data(data, threshold=5):\n",
        "    median = np.median(data)\n",
        "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
        "    lower_bound = median - threshold * iqr\n",
        "    upper_bound = median + threshold * iqr\n",
        "    cleaned_data = np.where((data < lower_bound) | (data > upper_bound), median, data)\n",
        "    print(f\"Nombre de valeurs aberrantes remplacées : {np.sum((data < lower_bound) | (data > upper_bound))}\")\n",
        "    return cleaned_data\n",
        "\n",
        "# Modèle de bruit classique pour le strain : S(f) = A f^(-2) + B f^2 + C\n",
        "def noise_model(f, A, B, C):\n",
        "    return A * f**(-2) + B * f**2 + C\n",
        "\n",
        "# Modèle alternatif pour détecter des anomalies : S(f) = A f^C + B\n",
        "def anomaly_model(f, A, B, C):\n",
        "    return A * f**C + B\n",
        "\n",
        "# Création du rapport PDF\n",
        "with PdfPages(\"rapport_analyse_bruit_gwosc_1187008877.pdf\") as pdf:\n",
        "    report_text = []\n",
        "\n",
        "    # Extraction des données\n",
        "    data = strain_data.value\n",
        "    print(f\"Données chargées : {data.size} échantillons\")\n",
        "\n",
        "    # Nettoyage des NaN et des valeurs aberrantes\n",
        "    valid_data = data[~np.isnan(data)]\n",
        "    cleaned_data = clean_data(valid_data)\n",
        "\n",
        "    # Calcul de la densité spectrale de puissance (PSD) avec Welch\n",
        "    frequencies, S_f = welch(cleaned_data, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "    # Filtrer les fréquences pertinentes (10-2000 Hz)\n",
        "    mask = (frequencies >= freq_min) & (frequencies <= freq_max)\n",
        "    frequencies = frequencies[mask]\n",
        "    S_f = S_f[mask]\n",
        "\n",
        "    # Normalisation de S(f) par la médiane\n",
        "    S_f_norm = S_f / np.median(S_f)\n",
        "    print(f\"Valeur de normalisation (médiane) : {np.median(S_f)}\")\n",
        "    print(f\"Valeur de normalisation (maximum) : {np.max(S_f)}\")\n",
        "\n",
        "    # Vérification des données avant le tracé\n",
        "    if np.any(frequencies <= 0) or np.any(S_f_norm <= 0):\n",
        "        raise ValueError(\"Les fréquences ou S_f_norm contiennent des valeurs non positives, ce qui est invalide pour un tracé logarithmique.\")\n",
        "    print(f\"Min fréquence : {np.min(frequencies)}, Max fréquence : {np.max(frequencies)}\")\n",
        "    print(f\"Min S_f_norm : {np.min(S_f_norm)}, Max S_f_norm : {np.max(S_f_norm)}\")\n",
        "\n",
        "    # Ajustement du modèle classique\n",
        "    p0 = [1e-40, 1e-48, 1e-45]  # Valeurs initiales pour [A, B, C]\n",
        "    bounds = ([0, 0, 0], [np.inf, np.inf, np.inf])\n",
        "    try:\n",
        "        popt, pcov = curve_fit(noise_model, frequencies, S_f_norm, p0=p0, bounds=bounds, maxfev=10000)\n",
        "        A_fit, B_fit, C_fit = popt\n",
        "        report_text.append(f\"Modèle classique ajusté : S(f) = {A_fit:.2e} * f^(-2) + {B_fit:.2e} * f^2 + {C_fit:.2e}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Erreur dans l’ajustement classique : {e}\")\n",
        "        A_fit, B_fit, C_fit = 1e-40, 0, 1e-45\n",
        "        report_text.append(f\"Échec de l’ajustement classique, valeurs par défaut : S(f) = {A_fit:.2e} * f^(-2) + {B_fit:.2e} * f^2 + {C_fit:.2e}\")\n",
        "\n",
        "    # Calcul du spectre ajusté (modèle classique)\n",
        "    S_f_background = noise_model(frequencies, A_fit, B_fit, C_fit)\n",
        "\n",
        "    # Ajustement du modèle alternatif (pour détecter des anomalies)\n",
        "    p0_alt = [1e-5, 1e-5, -1.0]  # Valeurs initiales ajustées\n",
        "    bounds_alt = ([0, 0, -5], [np.inf, np.inf, 5])\n",
        "    try:\n",
        "        popt_alt, pcov_alt = curve_fit(anomaly_model, frequencies, S_f_norm, p0=p0_alt, bounds=bounds_alt, maxfev=10000)\n",
        "        A_alt, B_alt, C_alt = popt_alt\n",
        "        report_text.append(f\"Modèle alternatif ajusté : S(f) = {A_alt:.2e} * f^{C_alt:.2f} + {B_alt:.2e}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Erreur dans l’ajustement alternatif : {e}\")\n",
        "        A_alt, B_alt, C_alt = 1e-5, 1e-5, 1.0\n",
        "        report_text.append(f\"Échec de l’ajustement alternatif, valeurs par défaut : S(f) = {A_alt:.2e} * f^{C_alt:.2f} + {B_alt:.2e}\")\n",
        "\n",
        "    # Calcul du spectre ajusté (modèle alternatif)\n",
        "    S_f_anomaly = anomaly_model(frequencies, A_alt, B_alt, C_alt)\n",
        "\n",
        "    # Bootstrap pour estimer les incertitudes\n",
        "    n_bootstrap = 500\n",
        "    bootstrap_psds = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        sample = np.random.choice(cleaned_data, size=len(cleaned_data), replace=True)\n",
        "        _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "        psd_sample = psd_sample[mask] / np.median(psd_sample[mask])\n",
        "        bootstrap_psds.append(psd_sample)\n",
        "    bootstrap_psds = np.array(bootstrap_psds)\n",
        "    psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "    psd_std = np.std(bootstrap_psds, axis=0)\n",
        "    psd_std = np.where(psd_std == 0, 1e-10, psd_std)\n",
        "\n",
        "    # Calcul du chi2 pour le modèle classique\n",
        "    chi2_classic = np.sum(((S_f_norm - S_f_background) / psd_std) ** 2)\n",
        "    report_text.append(f\"Chi2 modèle classique : {chi2_classic:.2e}\")\n",
        "\n",
        "    # Calcul du chi2 pour le modèle alternatif\n",
        "    chi2_anomaly = np.sum(((S_f_norm - S_f_anomaly) / psd_std) ** 2)\n",
        "    report_text.append(f\"Chi2 modèle alternatif : {chi2_anomaly:.2e}\")\n",
        "\n",
        "    # Simulation Monte Carlo pour la p-valeur (modèle classique)\n",
        "    n_simulations = 100\n",
        "    chi2_simulated = []\n",
        "    for _ in range(n_simulations):\n",
        "        noise = np.random.normal(0, 1, size=len(cleaned_data)) * np.sqrt(np.mean(S_f))\n",
        "        _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "        psd_sim = psd_sim[mask] / np.median(psd_sim[mask])\n",
        "        chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "        chi2_simulated.append(chi2_val)\n",
        "    p_value_classic = np.mean(np.array(chi2_simulated) > chi2_classic)\n",
        "    report_text.append(f\"P-valeur modèle classique : {p_value_classic:.3f}\")\n",
        "\n",
        "    # Graphique 1 : Spectre observé vs ajusté (approche explicite)\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.loglog(frequencies, S_f_norm, label=\"S(f) observé (normalisé)\")\n",
        "    ax.loglog(frequencies, S_f_background, label=\"S(f) classique ajusté\", linestyle=\"--\")\n",
        "    ax.loglog(frequencies, S_f_anomaly, label=\"S(f) alternatif ajusté\", linestyle=\":\")\n",
        "    ax.fill_between(frequencies, psd_mean - 2*psd_std, psd_mean + 2*psd_std, alpha=0.2, label=\"Intervalle 95% (Bootstrap)\")\n",
        "    ax.set_xlabel(\"Fréquence (Hz)\")\n",
        "    ax.set_ylabel(\"Densité spectrale S(f) (normalisée)\")\n",
        "    ax.set_title(\"Spectre de bruit (GWOSC, 10s, 1187008877)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True, which=\"both\", ls=\"--\")\n",
        "    pdf.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Graphique 2 : Série temporelle (pour rechercher un signal d’onde gravitationnelle)\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    ax.plot(strain_data.times.value, strain_data.value, label=\"Strain\")\n",
        "    ax.set_xlabel(\"Temps (s)\")\n",
        "    ax.set_ylabel(\"Strain\")\n",
        "    ax.set_title(\"Série temporelle (GWOSC, 10s, 1187008877)\")\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    pdf.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Page texte du rapport\n",
        "    fig, ax = plt.subplots(figsize=(8, 11))\n",
        "    ax.text(0.1, 0.95, \"Rapport d’analyse (GWOSC, 1187008877)\", fontsize=16, fontweight='bold')\n",
        "    ax.text(0.1, 0.9, \"\\n\".join(report_text), fontsize=12, verticalalignment='top')\n",
        "    ax.axis('off')\n",
        "    pdf.savefig(fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "    # Affichage console\n",
        "    print(\"\\n=== Rapport d’analyse ===\")\n",
        "    for line in report_text:\n",
        "        print(line)\n",
        "    print(\"=== Fin du Rapport ===\")"
      ],
      "metadata": {
        "id": "dk6Ww8caDZLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c1f67d-9e3f-4eec-aa35-495f7afe3df7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Données chargées : 40960 échantillons\n",
            "Nombre de valeurs aberrantes remplacées : 0\n",
            "Valeur de normalisation (médiane) : 1.9447821116294995e-43\n",
            "Valeur de normalisation (maximum) : 2.6745767256730534e-43\n",
            "Min fréquence : 12.0, Max fréquence : 2000.0\n",
            "Min S_f_norm : 0.6733420190935852, Max S_f_norm : 1.3752577780716377\n",
            "\n",
            "=== Rapport d’analyse ===\n",
            "Modèle classique ajusté : S(f) = 2.78e-14 * f^(-2) + 1.73e-09 * f^2 + 1.00e+00\n",
            "Modèle alternatif ajusté : S(f) = 1.08e-12 * f^-2.94 + 1.00e+00\n",
            "Chi2 modèle classique : 4.96e+02\n",
            "Chi2 modèle alternatif : 4.96e+02\n",
            "P-valeur modèle classique : 0.590\n",
            "=== Fin du Rapport ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "from gwpy.timeseries import TimeSeries\n",
        "\n",
        "# Paramètres des données\n",
        "n_samples = 40960  # 10 secondes à 4096 Hz\n",
        "sampling_rate = 4096  # Hz\n",
        "dt = 1 / sampling_rate  # Intervalle de temps entre échantillons\n",
        "\n",
        "# Simuler des données de strain (car la liste fournie est incomplète)\n",
        "# Remplacez cette ligne par vos données réelles si disponibles\n",
        "np.random.seed(42)  # Pour la reproductibilité\n",
        "strain_values = np.random.normal(loc=-4e-19, scale=2e-20, size=n_samples)  # Bruit centré autour de -4e-19\n",
        "\n",
        "# Création de l’objet TimeSeries\n",
        "strain_data = TimeSeries(\n",
        "    strain_values,\n",
        "    unit=None,\n",
        "    t0=1187008877.0,\n",
        "    dt=dt,\n",
        "    name='Strain',\n",
        "    channel=None\n",
        ")\n",
        "\n",
        "# Paramètres pour l’analyse\n",
        "freq_min, freq_max = 10, 2000  # Plage de fréquences analysées (Hz), limitée par la fréquence de Nyquist (2048 Hz)\n",
        "\n",
        "# Nettoyage des données : suppression des valeurs aberrantes\n",
        "def clean_data(data, threshold=5):\n",
        "    median = np.median(data)\n",
        "    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n",
        "    lower_bound = median - threshold * iqr\n",
        "    upper_bound = median + threshold * iqr\n",
        "    cleaned_data = np.where((data < lower_bound) | (data > upper_bound), median, data)\n",
        "    print(f\"Nombre de valeurs aberrantes remplacées : {np.sum((data < lower_bound) | (data > upper_bound))}\")\n",
        "    return cleaned_data\n",
        "\n",
        "# Modèle de bruit classique pour le strain : S(f) = A f^(-2) + B f^2 + C\n",
        "def noise_model(f, A, B, C):\n",
        "    return A * f**(-2) + B * f**2 + C\n",
        "\n",
        "# Modèle alternatif pour détecter des anomalies : S(f) = A f^C + B\n",
        "def anomaly_model(f, A, B, C):\n",
        "    return A * f**C + B\n",
        "\n",
        "# Création du rapport PDF\n",
        "with PdfPages(\"rapport_analyse_bruit_gwosc_1187008877.pdf\") as pdf:\n",
        "    report_text = []\n",
        "\n",
        "    # Extraction des données\n",
        "    data = strain_data.value\n",
        "    print(f\"Données chargées : {data.size} échantillons\")\n",
        "\n",
        "    # Nettoyage des NaN et des valeurs aberrantes\n",
        "    valid_data = data[~np.isnan(data)]\n",
        "    cleaned_data = clean_data(valid_data)\n",
        "\n",
        "    # Calcul de la densité spectrale de puissance (PSD) avec Welch\n",
        "    frequencies, S_f = welch(cleaned_data, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "    # Filtrer les fréquences pertinentes (10-2000 Hz)\n",
        "    mask = (frequencies >= freq_min) & (frequencies <= freq_max)\n",
        "    frequencies = frequencies[mask]\n",
        "    S_f = S_f[mask]\n",
        "\n",
        "    # Normalisation de S(f) par la médiane\n",
        "    S_f_norm = S_f / np.median(S_f)\n",
        "    print(f\"Valeur de normalisation (médiane) : {np.median(S_f)}\")\n",
        "    print(f\"Valeur de normalisation (maximum) : {np.max(S_f)}\")\n",
        "\n",
        "    # Vérification des données avant le tracé\n",
        "    if np.any(frequencies <= 0) or np.any(S_f_norm <= 0):\n",
        "        raise ValueError(\"Les fréquences ou S_f_norm contiennent des valeurs non positives, ce qui est invalide pour un tracé logarithmique.\")\n",
        "\n",
        "    # Ajustement du modèle classique\n",
        "    p0 = [1e-40, 1e-48, 1e-45]  # Valeurs initiales pour [A, B, C]\n",
        "    bounds = ([0, 0, 0], [np.inf, np.inf, np.inf])\n",
        "    try:\n",
        "        popt, pcov = curve_fit(noise_model, frequencies, S_f_norm, p0=p0, bounds=bounds, maxfev=10000)\n",
        "        A_fit, B_fit, C_fit = popt\n",
        "        report_text.append(f\"Modèle classique ajusté : S(f) = {A_fit:.2e} * f^(-2) + {B_fit:.2e} * f^2 + {C_fit:.2e}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Erreur dans l’ajustement classique : {e}\")\n",
        "        A_fit, B_fit, C_fit = 1e-40, 0, 1e-45\n",
        "        report_text.append(f\"Échec de l’ajustement classique, valeurs par défaut : S(f) = {A_fit:.2e} * f^(-2) + {B_fit:.2e} * f^2 + {C_fit:.2e}\")\n",
        "\n",
        "    # Calcul du spectre ajusté (modèle classique)\n",
        "    S_f_background = noise_model(frequencies, A_fit, B_fit, C_fit)\n",
        "\n",
        "    # Ajustement du modèle alternatif (pour détecter des anomalies)\n",
        "    p0_alt = [1e-5, 1e-5, -1.0]  # Valeurs initiales ajustées\n",
        "    bounds_alt = ([0, 0, -5], [np.inf, np.inf, 5])\n",
        "    try:\n",
        "        popt_alt, pcov_alt = curve_fit(anomaly_model, frequencies, S_f_norm, p0=p0_alt, bounds=bounds_alt, maxfev=10000)\n",
        "        A_alt, B_alt, C_alt = popt_alt\n",
        "        report_text.append(f\"Modèle alternatif ajusté : S(f) = {A_alt:.2e} * f^{C_alt:.2f} + {B_alt:.2e}\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Erreur dans l’ajustement alternatif : {e}\")\n",
        "        A_alt, B_alt, C_alt = 1e-5, 1e-5, 1.0\n",
        "        report_text.append(f\"Échec de l’ajustement alternatif, valeurs par défaut : S(f) = {A_alt:.2e} * f^{C_alt:.2f} + {B_alt:.2e}\")\n",
        "\n",
        "    # Calcul du spectre ajusté (modèle alternatif)\n",
        "    S_f_anomaly = anomaly_model(frequencies, A_alt, B_alt, C_alt)\n",
        "\n",
        "    # Bootstrap pour estimer les incertitudes\n",
        "    n_bootstrap = 500\n",
        "    bootstrap_psds = []\n",
        "    for _ in range(n_bootstrap):\n",
        "        sample = np.random.choice(cleaned_data, size=len(cleaned_data), replace=True)\n",
        "        _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "        psd_sample = psd_sample[mask] / np.median(psd_sample[mask])\n",
        "        bootstrap_psds.append(psd_sample)\n",
        "    bootstrap_psds = np.array(bootstrap_psds)\n",
        "    psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "    psd_std = np.std(bootstrap_psds, axis=0)\n",
        "    psd_std = np.where(psd_std == 0, 1e-10, psd_std)\n",
        "\n",
        "    # Calcul du chi2 pour le modèle classique\n",
        "    chi2_classic = np.sum(((S_f_norm - S_f_background) / psd_std) ** 2)\n",
        "    report_text.append(f\"Chi2 modèle classique : {chi2_classic:.2e}\")\n",
        "\n",
        "    # Calcul du chi2 pour le modèle alternatif\n",
        "    chi2_anomaly = np.sum(((S_f_norm - S_f_anomaly) / psd_std) ** 2)\n",
        "    report_text.append(f\"Chi2 modèle alternatif : {chi2_anomaly:.2e}\")\n",
        "\n",
        "    # Simulation Monte Carlo pour la p-valeur (modèle classique)\n",
        "    n_simulations = 100\n",
        "    chi2_simulated = []\n",
        "    for _ in range(n_simulations):\n",
        "        noise = np.random.normal(0, 1, size=len(cleaned_data)) * np.sqrt(np.mean(S_f))\n",
        "        _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "        psd_sim = psd_sim[mask] / np.median(psd_sim[mask])\n",
        "        chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "        chi2_simulated.append(chi2_val)\n",
        "    p_value_classic = np.mean(np.array(chi2_simulated) > chi2_classic)\n",
        "    report_text.append(f\"P-valeur modèle classique : {p_value_classic:.3f}\")\n",
        "\n",
        "    # Graphique 1 : Spectre observé vs ajusté\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.loglog(frequencies, S_f_norm, label=\"S(f) observé (normalisé)\")\n",
        "    plt.loglog(frequencies, S_f_background, label=\"S(f) classique ajusté\", linestyle=\"--\")\n",
        "    plt.loglog(frequencies, S_f_anomaly, label=\"S(f) alternatif ajusté\", linestyle=\":\")\n",
        "    plt.fill_between(frequencies, psd_mean - 2*psd_std, psd_mean + 2*psd_std, alpha=0.2, label=\"Intervalle 95% (Bootstrap)\")\n",
        "    plt.xlabel(\"Fréquence (Hz)\")\n",
        "    plt.ylabel(\"Densité spectrale S(f) (normalisée)\")\n",
        "    plt.title(\"Spectre de bruit (GWOSC, 10s, 1187008877)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, which=\"both\", ls=\"--\")\n",
        "    pdf.savefig()\n",
        "    plt.close()\n",
        "\n",
        "    # Graphique 2 : Série temporelle (pour rechercher un signal d’onde gravitationnelle)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(strain_data.times.value, strain_data.value, label=\"Strain\")\n",
        "    plt.xlabel(\"Temps (s)\")\n",
        "    plt.ylabel(\"Strain\")\n",
        "    plt.title(\"Série temporelle (GWOSC, 10s, 1187008877)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    pdf.savefig()\n",
        "    plt.close()\n",
        "\n",
        "    # Page texte du rapport\n",
        "    plt.figure(figsize=(8, 11))\n",
        "    plt.text(0.1, 0.95, \"Rapport d’analyse (GWOSC, 1187008877)\", fontsize=16, fontweight='bold')\n",
        "    plt.text(0.1, 0.9, \"\\n\".join(report_text), fontsize=12, verticalalignment='top')\n",
        "    plt.axis('off')\n",
        "    pdf.savefig()\n",
        "    plt.close()\n",
        "\n",
        "    # Affichage console\n",
        "    print(\"\\n=== Rapport d’analyse ===\")\n",
        "    for line in report_text:\n",
        "        print(line)\n",
        "    print(\"=== Fin du Rapport ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHIuGobsBg7f",
        "outputId": "accce480-02b1-47f5-f272-a85b8fe4b433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Données chargées : 40960 échantillons\n",
            "Nombre de valeurs aberrantes remplacées : 0\n",
            "Valeur de normalisation (médiane) : 1.9447821116294995e-43\n",
            "Valeur de normalisation (maximum) : 2.6745767256730534e-43\n",
            "\n",
            "=== Rapport d’analyse ===\n",
            "Modèle classique ajusté : S(f) = 2.78e-14 * f^(-2) + 1.73e-09 * f^2 + 1.00e+00\n",
            "Modèle alternatif ajusté : S(f) = 1.08e-12 * f^-2.94 + 1.00e+00\n",
            "Chi2 modèle classique : 4.96e+02\n",
            "Chi2 modèle alternatif : 4.96e+02\n",
            "P-valeur modèle classique : 0.590\n",
            "=== Fin du Rapport ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from gwpy.timeseries import TimeSeries\n",
        "\n",
        "\n",
        "def fetch_run_gps_times(run):\n",
        "    \"Fetch gwosc archive and return the (start, end) GPS time tuple of the run.\"\n",
        "    response = requests.get(\"https://gwosc.org/archive/all/json/\").json()\n",
        "    runs = response[\"runs\"]\n",
        "    run_info = runs.get(run)\n",
        "    if run_info is None:\n",
        "        raise ValueError(f\"Could not find run {run}. Available runs: {runs.keys()}\")\n",
        "    return run_info[\"GPSstart\"], run_info[\"GPSend\"]\n",
        "\n",
        "\n",
        "def fetch_strain_list(run, detector, gps_start=None, gps_end=None):\n",
        "    \"Return the list of strain file info for `run` and `detector`.\"\n",
        "    if gps_start is None or gps_end is None:\n",
        "        start, end = fetch_run_gps_times(run)\n",
        "        gps_start = gps_start or start\n",
        "        gps_end = gps_end or end\n",
        "\n",
        "    # Get the strain list\n",
        "    fetch_url = (\n",
        "        f\"https://gwosc.org/archive/links/\"\n",
        "        f\"{run}/{detector}/{gps_start}/{gps_end}/json/\"\n",
        "    )\n",
        "    response = requests.get(fetch_url)\n",
        "    response.raise_for_status()\n",
        "    return response.json()[\"strain\"]\n",
        "\n",
        "\n",
        "def download_strain_file(download_url):\n",
        "    \"Download the strain file on the given url and save to disk.\"\n",
        "    # In the next line I parse the file name from the download url.\n",
        "    # Ideally, the file name should be grabbed from the\n",
        "    # Content-Disposition response header.\n",
        "    filename = download_url.split(\"/\")[-1]\n",
        "    with requests.get(download_url, stream=True) as r:\n",
        "        r.raise_for_status()\n",
        "        with open(filename, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "    return filename\n",
        "\n",
        "\n",
        "def main():\n",
        "    strain_files = fetch_strain_list(\"O3b_4KHZ_R1\", \"H1\")\n",
        "    try:\n",
        "        with open(\"filesdone.txt\", \"r\") as fp:\n",
        "            donelist = [f.strip() for f in fp.readlines()]\n",
        "    except FileNotFoundError:\n",
        "        donelist = []\n",
        "    for afile in strain_files:\n",
        "        if afile[\"url\"] in donelist:\n",
        "            continue\n",
        "        if afile[\"format\"] == \"hdf5\":\n",
        "            print(f\"Downloading {afile['url']}\")\n",
        "            fname = download_strain_file(afile[\"url\"])\n",
        "            tseries = TimeSeries.read(fname, format=\"hdf5.gwosc\")\n",
        "            with open(\"filesdone.txt\", \"a\") as fp:\n",
        "                fp.write(f\"{afile['url']}\\n\")\n",
        "            # process tseries here\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nN-Vyxz9hAVs",
        "outputId": "618137f9-6a43-4faf-bbc4-ea1adf63c70b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256660992-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256665088-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256669184-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256673280-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256677376-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256681472-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256685568-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256689664-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256693760-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256697856-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256701952-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256710144-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256714240-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256718336-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256722432-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256726528-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256730624-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256734720-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256738816-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256742912-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256747008-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256751104-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256755200-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256759296-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256763392-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256767488-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256771584-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256775680-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256779776-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256783872-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256787968-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256792064-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256796160-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256800256-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256804352-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256808448-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256812544-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256816640-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256820736-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256824832-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256828928-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256833024-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256837120-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256841216-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256845312-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256849408-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256853504-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256857600-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256861696-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256865792-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256869888-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256873984-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256878080-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256882176-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256886272-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256890368-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256894464-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256898560-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256902656-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256906752-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256910848-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256914944-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256919040-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256923136-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256927232-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256931328-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256935424-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256951808-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256955904-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256960000-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256964096-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256968192-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256972288-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256976384-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256980480-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256984576-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256988672-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256992768-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256996864-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1257000960-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1257025536-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1257029632-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1257033728-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1257037824-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1257041920-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1257046016-4096.hdf5\n",
            "Downloading http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1257050112-4096.hdf5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-725384ba2a89>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-725384ba2a89>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mafile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hdf5\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Downloading {afile['url']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_strain_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mtseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hdf5.gwosc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesdone.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-725384ba2a89>\u001b[0m in \u001b[0;36mdownload_strain_file\u001b[0;34m(download_url)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     def _raw_read(\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "\n",
        "# Liste d'URLs (par exemple, les 3 premières pour tester)\n",
        "urls = [\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256660992-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256665088-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256669184-4096.hdf5\"\n",
        "]\n",
        "\n",
        "# Fonction pour lire un fichier HDF5 depuis une URL\n",
        "def read_hdf5_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        file_like_object = BytesIO(response.content)\n",
        "        with h5py.File(file_like_object, 'r') as f:\n",
        "            if 'strain' in f and 'Strain' in f['strain']:\n",
        "                data = f['strain/Strain'][:]\n",
        "                print(f\"Données lues depuis {url} : {data.size} échantillons\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"Dataset 'strain/Strain' non trouvé dans {url}\")\n",
        "                return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erreur lors du téléchargement de {url} : {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur inattendue pour {url} : {e}\")\n",
        "        return None\n",
        "\n",
        "# Lire et combiner les données de plusieurs fichiers\n",
        "combined_data = []\n",
        "for url in urls:\n",
        "    data = read_hdf5_from_url(url)\n",
        "    if data is not None:\n",
        "        combined_data.append(data)\n",
        "\n",
        "# Concaténer les données si elles existent\n",
        "if combined_data:\n",
        "    combined_data = np.concatenate(combined_data)\n",
        "    print(\"Attribution : Data from GWOSC, released under CC BY 4.0 License\")\n",
        "    print(\"Taille totale des données combinées :\", combined_data.size)\n",
        "    print(\"Échantillon des premières valeurs :\", combined_data[:10])\n",
        "else:\n",
        "    print(\"Aucune donnée n'a pu être lue.\")"
      ],
      "metadata": {
        "id": "YrllnbKNu2tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "\n",
        "# Liste des URLs\n",
        "urls = [\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256660992-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256665088-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256669184-4096.hdf5\"\n",
        "]\n",
        "\n",
        "# Fonction pour lire un fichier HDF5 depuis une URL\n",
        "def read_hdf5_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        file_like_object = BytesIO(response.content)\n",
        "        with h5py.File(file_like_object, 'r') as f:\n",
        "            if 'strain' in f and 'Strain' in f['strain']:\n",
        "                data = f['strain/Strain'][:]\n",
        "                print(f\"Données lues depuis {url} : {data.size} échantillons\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"Dataset 'strain/Strain' non trouvé dans {url}\")\n",
        "                return None\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erreur lors du téléchargement de {url} : {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur inattendue pour {url} : {e}\")\n",
        "        return None\n",
        "\n",
        "# Lire et combiner les données\n",
        "combined_data = []\n",
        "for url in urls:\n",
        "    data = read_hdf5_from_url(url)\n",
        "    if data is not None:\n",
        "        combined_data.append(data)\n",
        "\n",
        "# Vérifications sur les données combinées\n",
        "if combined_data:\n",
        "    combined_data = np.concatenate(combined_data)\n",
        "    print(\"Attribution : Data from GWOSC, released under CC BY 4.0 License\")\n",
        "    print(\"Taille totale des données combinées :\", combined_data.size)\n",
        "\n",
        "    # Vérifier la proportion de NaN\n",
        "    nan_count = np.isnan(combined_data).sum()\n",
        "    print(f\"Nombre de valeurs NaN : {nan_count} ({100 * nan_count / combined_data.size:.2f}%)\")\n",
        "\n",
        "    # Afficher des échantillons à différents endroits\n",
        "    print(\"Échantillon des premières valeurs :\", combined_data[:10])\n",
        "    print(\"Échantillon au milieu :\", combined_data[combined_data.size//2:combined_data.size//2 + 10])\n",
        "    print(\"Échantillon des dernières valeurs :\", combined_data[-10:])\n",
        "\n",
        "    # Si tout est NaN, arrêter ici\n",
        "    if nan_count == combined_data.size:\n",
        "        print(\"Toutes les données sont NaN. Vérifiez les fichiers ou la structure des données.\")\n",
        "    else:\n",
        "        # Trouver la première valeur non-NaN pour inspection\n",
        "        non_nan_indices = np.where(~np.isnan(combined_data))[0]\n",
        "        if len(non_nan_indices) > 0:\n",
        "            first_non_nan_idx = non_nan_indices[0]\n",
        "            print(f\"Première valeur non-NaN à l'index {first_non_nan_idx} : {combined_data[first_non_nan_idx]}\")\n",
        "else:\n",
        "    print(\"Aucune donnée n'a pu être lue.\")"
      ],
      "metadata": {
        "id": "H6ms8puHvtR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtrer les NaN\n",
        "valid_data = combined_data[~np.isnan(combined_data)]\n",
        "print(\"Taille des données valides :\", valid_data.size)\n",
        "print(\"Échantillon des premières valeurs valides :\", valid_data[:10])\n",
        "\n",
        "# Calculer la durée des données valides (en secondes)\n",
        "sampling_rate = 4000  # 4 kHz\n",
        "duration_valid = valid_data.size / sampling_rate\n",
        "print(f\"Durée des données valides : {duration_valid:.2f} secondes\")\n",
        "# Timestamp de début du premier fichier\n",
        "start_time_gps = 1256660992\n",
        "\n",
        "# Calculer le temps correspondant au premier échantillon non-NaN\n",
        "nan_duration = 10039296 / sampling_rate  # Durée des NaN en secondes\n",
        "start_valid_time_gps = start_time_gps + nan_duration\n",
        "print(f\"Les données valides commencent à GPS : {start_valid_time_gps:.2f}\")\n",
        "\n",
        "# Créer un tableau de temps pour les données valides\n",
        "time_valid = np.arange(0, valid_data.size) / sampling_rate + start_valid_time_gps\n",
        "print(\"Échantillon des premiers timestamps :\", time_valid[:10])\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tracer une petite portion des données valides (par exemple, 1 seconde)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(time_valid[:4000], valid_data[:4000], label=\"Strain H1\")\n",
        "plt.xlabel(\"Temps GPS (s)\")\n",
        "plt.ylabel(\"Strain\")\n",
        "plt.title(\"Données de déformation (H1) - 1 seconde\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "agdnCpBwxZUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import welch\n",
        "from scipy.stats import chi2\n",
        "\n",
        "# Données valides à partir de votre sortie précédente\n",
        "sampling_rate = 4000  # 4 kHz\n",
        "valid_data = combined_data[~np.isnan(combined_data)]  # 40 292 352 échantillons\n",
        "start_valid_time_gps = 1256663501.82\n",
        "time_valid = np.arange(0, valid_data.size) / sampling_rate + start_valid_time_gps\n",
        "\n",
        "# 1. Calculer le spectre de bruit S(f) avec la méthode de Welch\n",
        "frequencies, psd = welch(valid_data, fs=sampling_rate, nperseg=4096, noverlap=2048)\n",
        "S_f = psd  # Densité spectrale de puissance\n",
        "\n",
        "# 2. Bootstrap pour estimer la variabilité de S(f)\n",
        "n_bootstrap = 500  # Réduit pour accélérer, augmentez si nécessaire\n",
        "bootstrap_psds = []\n",
        "for _ in range(n_bootstrap):\n",
        "    sample = np.random.choice(valid_data, size=len(valid_data), replace=True)\n",
        "    _, psd_sample = welch(sample, fs=sampling_rate, nperseg=4096, noverlap=2048)\n",
        "    bootstrap_psds.append(psd_sample)\n",
        "\n",
        "bootstrap_psds = np.array(bootstrap_psds)\n",
        "psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "psd_std = np.std(bootstrap_psds, axis=0)\n",
        "psd_ci_lower = np.percentile(bootstrap_psds, 2.5, axis=0)  # 95% CI\n",
        "psd_ci_upper = np.percentile(bootstrap_psds, 97.5, axis=0)\n",
        "\n",
        "# 3. Simulations Monte Carlo pour \\(\\chi^2\\)\n",
        "n_simulations = 500  # Réduit pour accélérer\n",
        "chi2_simulated = []\n",
        "for _ in range(n_simulations):\n",
        "    noise = np.random.normal(0, 1, size=len(valid_data)) * np.sqrt(np.mean(S_f))\n",
        "    _, psd_sim = welch(noise, fs=sampling_rate, nperseg=4096, noverlap=2048)\n",
        "    chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)  # \\(\\chi^2\\) simulé\n",
        "    chi2_simulated.append(chi2_val)\n",
        "\n",
        "# 4. Comparaison avec \\(\\chi^2 \\sim 10^{21}\\)\n",
        "chi2_observed = 1e21  # Votre valeur observée\n",
        "p_value_mc = np.mean(np.array(chi2_simulated) > chi2_observed)\n",
        "print(f\"P-valeur Monte Carlo pour \\(\\chi^2 = {chi2_observed}\\): {p_value_mc}\")\n",
        "\n",
        "# Visualisation du spectre\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.loglog(frequencies, S_f, label=\"S(f) observé\")\n",
        "plt.fill_between(frequencies, psd_ci_lower, psd_ci_upper, alpha=0.2, label=\"Intervalle de confiance 95% (Bootstrap)\")\n",
        "plt.xlabel(\"Fréquence (Hz)\")\n",
        "plt.ylabel(\"Densité spectrale de puissance S(f)\")\n",
        "plt.title(\"Spectre de bruit (H1, GPS 1256663501.82 et suivants)\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "plt.show()\n",
        "\n",
        "# Histogramme des \\(\\chi^2\\) simulés\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(chi2_simulated, bins=50, density=True, alpha=0.7, label=\"\\(\\chi^2\\) simulés\")\n",
        "plt.axvline(chi2_observed, color=\"red\", linestyle=\"--\", label=f\"\\(\\chi^2\\) observé = {chi2_observed:.2e}\")\n",
        "plt.xlabel(\"\\(\\chi^2\\)\")\n",
        "plt.ylabel(\"Densité\")\n",
        "plt.title(\"Distribution des \\(\\chi^2\\) simulés\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Vérification des écarts extrêmes\n",
        "max_chi2_simulated = np.max(chi2_simulated)\n",
        "print(f\"Maximum \\(\\chi^2\\) simulé : {max_chi2_simulated:.2e}\")\n",
        "if max_chi2_simulated < chi2_observed:\n",
        "    print(\"Le \\(\\chi^2\\) observé dépasse largement les fluctuations statistiques simulées.\")"
      ],
      "metadata": {
        "id": "nXrpPSYXzoxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import welch\n",
        "from scipy.stats import chi2\n",
        "\n",
        "# Données valides\n",
        "sampling_rate = 4000  # 4 kHz\n",
        "valid_data = combined_data[~np.isnan(combined_data)]  # 40 292 352 échantillons\n",
        "\n",
        "# Réduire la taille des données (par exemple, premières 1000 secondes = 4 000 000 échantillons)\n",
        "subset_size = 4000000\n",
        "valid_data_subset = valid_data[:subset_size]\n",
        "start_valid_time_gps = 1256663501.82\n",
        "time_valid = np.arange(0, valid_data_subset.size) / sampling_rate + start_valid_time_gps\n",
        "\n",
        "# 1. Calculer le spectre de bruit S(f)\n",
        "frequencies, psd = welch(valid_data_subset, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "S_f = psd\n",
        "\n",
        "# 2. Bootstrap (réduit à 100 itérations)\n",
        "n_bootstrap = 100\n",
        "bootstrap_psds = []\n",
        "for _ in range(n_bootstrap):\n",
        "    sample = np.random.choice(valid_data_subset, size=len(valid_data_subset), replace=True)\n",
        "    _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "    bootstrap_psds.append(psd_sample)\n",
        "\n",
        "bootstrap_psds = np.array(bootstrap_psds)\n",
        "psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "psd_std = np.std(bootstrap_psds, axis=0)\n",
        "psd_ci_lower = np.percentile(bootstrap_psds, 2.5, axis=0)\n",
        "psd_ci_upper = np.percentile(bootstrap_psds, 97.5, axis=0)\n",
        "\n",
        "# 3. Simulations Monte Carlo (réduit à 100 itérations)\n",
        "n_simulations = 100\n",
        "chi2_simulated = []\n",
        "for _ in range(n_simulations):\n",
        "    noise = np.random.normal(0, 1, size=len(valid_data_subset)) * np.sqrt(np.mean(S_f))\n",
        "    _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "    chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "    chi2_simulated.append(chi2_val)\n",
        "\n",
        "# 4. Comparaison avec \\(\\chi^2 \\sim 10^{21}\\)\n",
        "chi2_observed = 1e21\n",
        "p_value_mc = np.mean(np.array(chi2_simulated) > chi2_observed)\n",
        "print(f\"P-valeur Monte Carlo pour \\(\\chi^2 = {chi2_observed}\\): {p_value_mc}\")\n",
        "print(f\"Maximum \\(\\chi^2\\) simulé : {np.max(chi2_simulated):.2e}\")\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.loglog(frequencies, S_f, label=\"S(f) observé (1000s)\")\n",
        "plt.fill_between(frequencies, psd_ci_lower, psd_ci_upper, alpha=0.2, label=\"Intervalle de confiance 95%\")\n",
        "plt.xlabel(\"Fréquence (Hz)\")\n",
        "plt.ylabel(\"Densité spectrale de puissance S(f)\")\n",
        "plt.title(\"Spectre de bruit (H1, premières 1000s à partir de GPS 1256663501.82)\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(chi2_simulated, bins=30, density=True, alpha=0.7, label=\"\\(\\chi^2\\) simulés\")\n",
        "plt.axvline(chi2_observed, color=\"red\", linestyle=\"--\", label=f\"\\(\\chi^2\\) observé = {chi2_observed:.2e}\")\n",
        "plt.xlabel(\"\\(\\chi^2\\)\")\n",
        "plt.ylabel(\"Densité\")\n",
        "plt.title(\"Distribution des \\(\\chi^2\\) simulés\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-lCzEEfj2GVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scpy\n",
        "from scpy.timeseries import TimeSeries"
      ],
      "metadata": {
        "id": "YMLiH_SxUmV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy\n",
        "from scipy import stats\n",
        "from scipy import signal\n",
        "#TimeSeries was removed in newer versions of scipy\n",
        "#Use pandas for timeseries analysis\n",
        "#from scipy.stats import chi2"
      ],
      "metadata": {
        "id": "zT3njWctal8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gwpy numpy scipy matplotlib h5py requests\n",
        "!python3 quantum_gw_analysis.py"
      ],
      "metadata": {
        "id": "-Qh7Lh4mdXbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import h5py\n",
        "\n",
        "# Fonction pour lire un fichier HDF5 depuis une URL\n",
        "def read_hdf5_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        file_like_object = BytesIO(response.content)\n",
        "        with h5py.File(file_like_object, 'r') as f:\n",
        "            if 'strain' in f and 'Strain' in f['strain']:\n",
        "                data = f['strain/Strain'][:]\n",
        "                print(f\"Données lues depuis {url} : {data.size} échantillons\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"Dataset 'strain/Strain' non trouvé dans {url}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour {url} : {e}\")\n",
        "        return None\n",
        "\n",
        "# Paramètres globaux\n",
        "sampling_rate = 4000  # 4 kHz\n",
        "start_valid_time_gps = 1256663501.82\n",
        "\n",
        "# Créer un PDF pour le rapport unifié\n",
        "with PdfPages(\"rapport_unifie.pdf\") as pdf:\n",
        "    # Étape 1 : Données actuelles (H1, O3b, premières 1000s)\n",
        "    url_current = \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256660992-4096.hdf5\"\n",
        "    combined_data = read_hdf5_from_url(url_current)\n",
        "    report_text = []\n",
        "\n",
        "    if combined_data is not None:\n",
        "        valid_data = combined_data[~np.isnan(combined_data)][:4000000]  # 1000s\n",
        "        time_valid = np.arange(0, valid_data.size) / sampling_rate + start_valid_time_gps\n",
        "        frequencies, S_f = welch(valid_data, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "        # Modèle logarithmique pour éviter les overflows\n",
        "        def log_noise_model(log_f, log_A, alpha):\n",
        "            return log_A - alpha * log_f\n",
        "\n",
        "        # Ajustement du modèle de bruit\n",
        "        mask = (frequencies > 10) & (frequencies < 1000)\n",
        "        log_f = np.log10(frequencies[mask])\n",
        "        log_S_f = np.log10(S_f[mask])\n",
        "        initial_alpha = -2\n",
        "        initial_log_A = np.log10(np.mean(S_f[mask] * frequencies[mask]**2))\n",
        "        p0 = [initial_log_A, initial_alpha]\n",
        "        popt, pcov = curve_fit(log_noise_model, log_f, log_S_f, p0=p0, maxfev=2000)\n",
        "        log_A_fit, alpha_fit = popt\n",
        "        A_fit = 10**log_A_fit\n",
        "        report_text.append(f\"1. Test de Bootstrap / Monte Carlo :\")\n",
        "        report_text.append(f\"   - Modèle ajusté : S(f) = {A_fit:.2e} * f^(-{alpha_fit:.2f})\")\n",
        "\n",
        "        # Bruit de fond et écarts (exclure f=0)\n",
        "        freq_mask = frequencies > 0  # Éviter la division par zéro\n",
        "        S_f_background = np.zeros_like(frequencies)\n",
        "        S_f_background[freq_mask] = A_fit * frequencies[freq_mask]**(-alpha_fit)\n",
        "        delta_S_f = S_f - S_f_background\n",
        "\n",
        "        # Bootstrap\n",
        "        n_bootstrap = 100\n",
        "        bootstrap_psds = []\n",
        "        for _ in range(n_bootstrap):\n",
        "            sample = np.random.choice(valid_data, size=len(valid_data), replace=True)\n",
        "            _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            bootstrap_psds.append(psd_sample)\n",
        "        bootstrap_psds = np.array(bootstrap_psds)\n",
        "        psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "        psd_std = np.std(bootstrap_psds, axis=0)\n",
        "        psd_ci_lower = np.percentile(bootstrap_psds, 2.5, axis=0)\n",
        "        psd_ci_upper = np.percentile(bootstrap_psds, 97.5, axis=0)\n",
        "\n",
        "        # Monte Carlo\n",
        "        n_simulations = 100\n",
        "        chi2_simulated = []\n",
        "        for _ in range(n_simulations):\n",
        "            noise = np.random.normal(0, 1, size=len(valid_data)) * np.sqrt(np.mean(S_f))\n",
        "            _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "            chi2_simulated.append(chi2_val)\n",
        "\n",
        "        # Figure 1 : Spectre de bruit\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.loglog(frequencies, S_f, label=\"S(f) observé\")\n",
        "        plt.fill_between(frequencies, psd_ci_lower, psd_ci_upper, alpha=0.2, label=\"Intervalle 95% (Bootstrap)\")\n",
        "        plt.loglog(frequencies[freq_mask], S_f_background[freq_mask], label=f\"S(f) ajusté (f^(-{alpha_fit:.2f}))\", linestyle=\"--\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(\"Densité spectrale S(f)\")\n",
        "        plt.title(\"Spectre de bruit (H1, 1000s à partir de GPS 1256663501.82)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 2 : Écarts \\delta S(f)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=r\"$\\delta S(f)$\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Écarts par rapport au bruit de fond\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 2 : Significativité\n",
        "        chi2_observed_adjusted = np.sum(((S_f - S_f_background) / psd_std) ** 2)\n",
        "        p_value_adjusted = np.mean(np.array(chi2_simulated) > chi2_observed_adjusted)\n",
        "        report_text.append(f\"2. Significativité :\")\n",
        "        report_text.append(f\"   - \\(\\chi^2\\) ajusté : {chi2_observed_adjusted:.2e}\")\n",
        "        report_text.append(f\"   - P-valeur ajustée : {p_value_adjusted}\")\n",
        "        thresholds = {\"p < 0.05\": 0.05, \"p < 0.01\": 0.01, \"p < 0.001\": 0.001}\n",
        "        for threshold_name, threshold_value in thresholds.items():\n",
        "            if p_value_adjusted < threshold_value:\n",
        "                report_text.append(f\"   - L’écart est significatif au seuil {threshold_name}\")\n",
        "\n",
        "        # Figure 3 : Distribution des \\(\\chi^2\\) simulés\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.hist(chi2_simulated, bins=30, density=True, alpha=0.7, label=\"\\(\\chi^2\\) simulés\")\n",
        "        plt.axvline(chi2_observed_adjusted, color=\"red\", linestyle=\"--\", label=f\"\\(\\chi^2\\) ajusté = {chi2_observed_adjusted:.2e}\")\n",
        "        plt.xlabel(\"\\(\\chi^2\\)\")\n",
        "        plt.ylabel(\"Densité\")\n",
        "        plt.title(\"Distribution des \\(\\chi^2\\) simulés\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 3 : Analyse comparative\n",
        "        events = {\n",
        "            \"GW190521\": (1242442967, \"http://gwosc.org/archive/data/O2_4KHZ_R1/1242174976/H-H1_GWOSC_O2_4KHZ_R1-1242442944-4096.hdf5\"),\n",
        "            \"GW190814\": (1249852257, \"http://gwosc.org/archive/data/O2_4KHZ_R1/1249534464/H-H1_GWOSC_O2_4KHZ_R1-1249852160-4096.hdf5\"),\n",
        "            \"GW200129\": (1264282630, \"http://gwosc.org/archive/data/O3a_4KHZ_R1/1264129024/H-H1_GWOSC_O3a_4KHZ_R1-1264282624-4096.hdf5\")\n",
        "        }\n",
        "        results = {}\n",
        "        for event, (gps_time, url) in events.items():\n",
        "            data_event = read_hdf5_from_url(url)\n",
        "            if data_event is not None:\n",
        "                valid_data_event = data_event[~np.isnan(data_event)][:4000000]\n",
        "                freqs_event, S_f_event = welch(valid_data_event, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "                # Ajustement du modèle\n",
        "                popt_event, _ = curve_fit(log_noise_model, np.log10(freqs_event[mask]), np.log10(S_f_event[mask]), p0=p0, maxfev=2000)\n",
        "                log_A_event, alpha_event = popt_event\n",
        "                A_event = 10**log_A_event\n",
        "                S_f_background_event = np.zeros_like(freqs_event)\n",
        "                S_f_background_event[freq_mask] = A_event * freqs_event[freq_mask]**(-alpha_event)\n",
        "                delta_S_f_event = S_f_event - S_f_background_event\n",
        "\n",
        "                # Tester \\(\\delta S(f) \\propto f^2\\)\n",
        "                log_freqs_event = np.log10(freqs_event[mask])\n",
        "                log_delta_S_event = np.log10(np.abs(delta_S_f_event[mask]))\n",
        "                slope, _ = np.polyfit(log_freqs_event, log_delta_S_event, 1)\n",
        "                results[event] = {\"slope\": slope, \"delta_S_f\": delta_S_f_event}\n",
        "\n",
        "        # Figure 4 : Comparaison des \\(\\delta S(f)\\)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=\"H1 O3b (1256663501.82)\")\n",
        "        for event, result in results.items():\n",
        "            plt.semilogx(freqs_event, result[\"delta_S_f\"], label=f\"{event} (slope = {result['slope']:.2f})\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Comparaison des écarts \\(\\delta S(f)\\) pour différents événements\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Page texte pour le rapport\n",
        "        plt.figure(figsize=(8, 11))\n",
        "        plt.text(0.1, 0.95, \"Rapport Complet\", fontsize=16, fontweight='bold')\n",
        "        plt.text(0.1, 0.9, \"\\n\".join(report_text), fontsize=12, verticalalignment='top')\n",
        "        plt.axis('off')\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Rapport textuel dans la console\n",
        "        print(\"\\n=== Rapport Complet ===\")\n",
        "        for line in report_text:\n",
        "            print(line)\n",
        "        print(\"3. Analyse Comparative :\")\n",
        "        for event, result in results.items():\n",
        "            print(f\"   - {event} : Pente de \\(\\delta S(f)\\) = {result['slope']:.2f}\")\n",
        "        print(\"=== Fin du Rapport ===\")\n",
        "\n",
        "    else:\n",
        "        print(\"Échec du chargement des données initiales.\")"
      ],
      "metadata": {
        "id": "evbrnN_Z-xxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "import h5py\n",
        "\n",
        "# Paramètres globaux\n",
        "sampling_rate = 4000  # 4 kHz\n",
        "\n",
        "# Liste des chemins locaux des fichiers\n",
        "file_paths = [\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256669184-4096.hdf5\",\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256677376-4096.hdf5\",\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256685568-4096.hdf5\"\n",
        "]\n",
        "\n",
        "# Fonction pour lire un fichier HDF5 local\n",
        "def read_hdf5_local(file_path):\n",
        "    try:\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            if 'strain' in f and 'Strain' in f['strain']:\n",
        "                data = f['strain/Strain'][:]\n",
        "                print(f\"Données lues depuis {file_path} : {data.size} échantillons\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"Dataset 'strain/Strain' non trouvé dans {file_path}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour {file_path} : {e}\")\n",
        "        return None\n",
        "\n",
        "# Créer un PDF pour le rapport unifié\n",
        "with PdfPages(\"rapport_statistique_hypothese.pdf\") as pdf:\n",
        "    report_text = []\n",
        "    all_data = []\n",
        "\n",
        "    # Charger les trois fichiers\n",
        "    for file_path in file_paths:\n",
        "        data = read_hdf5_local(file_path)\n",
        "        if data is not None:\n",
        "            valid_data = data[~np.isnan(data)][:4000000]  # 1000s par fichier\n",
        "            all_data.append(valid_data)\n",
        "\n",
        "    if all_data:\n",
        "        # Étape 1 : Test de Bootstrap / Monte Carlo\n",
        "        combined_data = np.concatenate(all_data)  # Combiner les données des trois fichiers\n",
        "        frequencies, S_f = welch(combined_data, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "        # Modèle logarithmique\n",
        "        def log_noise_model(log_f, log_A, alpha):\n",
        "            return log_A - alpha * log_f\n",
        "\n",
        "        mask = (frequencies > 10) & (frequencies < 1000)\n",
        "        log_f = np.log10(frequencies[mask])\n",
        "        log_S_f = np.log10(S_f[mask])\n",
        "        initial_alpha = -2\n",
        "        initial_log_A = np.log10(np.mean(S_f[mask] * frequencies[mask]**2))\n",
        "        p0 = [initial_log_A, initial_alpha]\n",
        "        popt, pcov = curve_fit(log_noise_model, log_f, log_S_f, p0=p0, maxfev=2000)\n",
        "        log_A_fit, alpha_fit = popt\n",
        "        A_fit = 10**log_A_fit\n",
        "        report_text.append(f\"1. Test de Bootstrap / Monte Carlo :\")\n",
        "        report_text.append(f\"   - Modèle ajusté : S(f) = {A_fit:.2e} * f^(-{alpha_fit:.2f})\")\n",
        "\n",
        "        # Bruit de fond et écarts\n",
        "        freq_mask = frequencies > 0\n",
        "        S_f_background = np.zeros_like(frequencies)\n",
        "        S_f_background[freq_mask] = A_fit * frequencies[freq_mask]**(-alpha_fit)\n",
        "        delta_S_f = S_f - S_f_background\n",
        "\n",
        "        # Bootstrap\n",
        "        n_bootstrap = 100\n",
        "        bootstrap_psds = []\n",
        "        for _ in range(n_bootstrap):\n",
        "            sample = np.random.choice(combined_data, size=len(combined_data), replace=True)\n",
        "            _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            bootstrap_psds.append(psd_sample)\n",
        "        bootstrap_psds = np.array(bootstrap_psds)\n",
        "        psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "        psd_std = np.std(bootstrap_psds, axis=0)\n",
        "        psd_ci_lower = np.percentile(bootstrap_psds, 2.5, axis=0)\n",
        "        psd_ci_upper = np.percentile(bootstrap_psds, 97.5, axis=0)\n",
        "\n",
        "        # Monte Carlo\n",
        "        n_simulations = 100\n",
        "        chi2_simulated = []\n",
        "        for _ in range(n_simulations):\n",
        "            noise = np.random.normal(0, 1, size=len(combined_data)) * np.sqrt(np.mean(S_f))\n",
        "            _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "            chi2_simulated.append(chi2_val)\n",
        "\n",
        "        # Figure 1 : Spectre de bruit\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.loglog(frequencies, S_f, label=\"S(f) observé\")\n",
        "        plt.fill_between(frequencies, psd_ci_lower, psd_ci_upper, alpha=0.2, label=\"Intervalle 95% (Bootstrap)\")\n",
        "        plt.loglog(frequencies[freq_mask], S_f_background[freq_mask], label=f\"S(f) ajusté (f^(-{alpha_fit:.2f}))\", linestyle=\"--\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(\"Densité spectrale S(f)\")\n",
        "        plt.title(\"Spectre de bruit (H1, O3b combiné, 3000s)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 2 : Écarts \\delta S(f)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=r\"$\\delta S(f)$\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Écarts par rapport au bruit de fond\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 2 : Significativité\n",
        "        chi2_observed_adjusted = np.sum(((S_f - S_f_background) / psd_std) ** 2)\n",
        "        p_value_adjusted = np.mean(np.array(chi2_simulated) > chi2_observed_adjusted)\n",
        "        report_text.append(f\"2. Significativité :\")\n",
        "        report_text.append(f\"   - \\(\\chi^2\\) ajusté : {chi2_observed_adjusted:.2e}\")\n",
        "        report_text.append(f\"   - P-valeur ajustée : {p_value_adjusted}\")\n",
        "        thresholds = {\"p < 0.05\": 0.05, \"p < 0.01\": 0.01, \"p < 0.001\": 0.001}\n",
        "        for threshold_name, threshold_value in thresholds.items():\n",
        "            if p_value_adjusted < threshold_value:\n",
        "                report_text.append(f\"   - L’écart est significatif au seuil {threshold_name}\")\n",
        "\n",
        "        # Figure 3 : Distribution des \\(\\chi^2\\) simulés\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.hist(chi2_simulated, bins=30, density=True, alpha=0.7, label=\"\\(\\chi^2\\) simulés\")\n",
        "        plt.axvline(chi2_observed_adjusted, color=\"red\", linestyle=\"--\", label=f\"\\(\\chi^2\\) ajusté = {chi2_observed_adjusted:.2e}\")\n",
        "        plt.xlabel(\"\\(\\chi^2\\)\")\n",
        "        plt.ylabel(\"Densité\")\n",
        "        plt.title(\"Distribution des \\(\\chi^2\\) simulés\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 3 : Analyse comparative entre les trois fichiers\n",
        "        results = {}\n",
        "        for i, file_path in enumerate(file_paths):\n",
        "            data = read_hdf5_local(file_path)\n",
        "            if data is not None:\n",
        "                valid_data_file = data[~np.isnan(data)][:4000000]\n",
        "                freqs_file, S_f_file = welch(valid_data_file, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "                # Ajustement du modèle\n",
        "                popt_file, _ = curve_fit(log_noise_model, np.log10(freqs_file[mask]), np.log10(S_f_file[mask]), p0=p0, maxfev=2000)\n",
        "                log_A_file, alpha_file = popt_file\n",
        "                A_file = 10**log_A_file\n",
        "                S_f_background_file = np.zeros_like(freqs_file)\n",
        "                S_f_background_file[freq_mask] = A_file * freqs_file[freq_mask]**(-alpha_file)\n",
        "                delta_S_f_file = S_f_file - S_f_background_file\n",
        "\n",
        "                # Tester \\(\\delta S(f) \\propto f^2\\)\n",
        "                log_freqs_file = np.log10(freqs_file[mask])\n",
        "                log_delta_S_file = np.log10(np.abs(delta_S_f_file[mask]))\n",
        "                slope, _ = np.polyfit(log_freqs_file, log_delta_S_file, 1)\n",
        "                results[file_path] = {\"slope\": slope, \"delta_S_f\": delta_S_f_file}\n",
        "\n",
        "        # Figure 4 : Comparaison des \\(\\delta S(f)\\) pour les trois fichiers\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=\"Combiné (3000s)\")\n",
        "        for file_path, result in results.items():\n",
        "            plt.semilogx(freqs_file, result[\"delta_S_f\"], label=f\"{file_path.split('/')[-1]} (slope = {result['slope']:.2f})\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Comparaison des écarts \\(\\delta S(f)\\) pour les trois fichiers O3b\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Page texte pour le rapport\n",
        "        plt.figure(figsize=(8, 11))\n",
        "        plt.text(0.1, 0.95, \"Rapport Statistique selon Hypothèse Multidimensionnelle\", fontsize=16, fontweight='bold')\n",
        "        plt.text(0.1, 0.9, \"\\n\".join(report_text), fontsize=12, verticalalignment='top')\n",
        "        plt.axis('off')\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Rapport textuel dans la console\n",
        "        print(\"\\n=== Rapport Statistique ===\")\n",
        "        for line in report_text:\n",
        "            print(line)\n",
        "        print(\"3. Analyse Comparative :\")\n",
        "        for file_path, result in results.items():\n",
        "            print(f\"   - {file_path.split('/')[-1]} : Pente de \\(\\delta S(f)\\) = {result['slope']:.2f}\")\n",
        "        print(\"=== Fin du Rapport ===\")\n",
        "\n",
        "    else:\n",
        "        print(\"Échec du chargement des données.\")\n"
      ],
      "metadata": {
        "id": "ytFlSJEfENpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "import h5py\n",
        "\n",
        "# Paramètres globaux\n",
        "sampling_rate = 4000  # 4 kHz\n",
        "\n",
        "# Liste des chemins locaux des fichiers\n",
        "file_paths = [\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256669184-4096.hdf5\",\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256677376-4096.hdf5\",\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256685568-4096.hdf5\"\n",
        "]\n",
        "\n",
        "# Fonction pour lire un fichier HDF5 local\n",
        "def read_hdf5_local(file_path):\n",
        "    try:\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            if 'strain' in f and 'Strain' in f['strain']:\n",
        "                data = f['strain/Strain'][:]\n",
        "                print(f\"Données lues depuis {file_path} : {data.size} échantillons\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"Dataset 'strain/Strain' non trouvé dans {file_path}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour {file_path} : {e}\")\n",
        "        return None\n",
        "\n",
        "# Créer un PDF pour le rapport unifié\n",
        "with PdfPages(\"rapport_statistique_hypothese_ajuste.pdf\") as pdf:\n",
        "    report_text = []\n",
        "    all_data = []\n",
        "\n",
        "    # Charger les trois fichiers\n",
        "    for file_path in file_paths:\n",
        "        data = read_hdf5_local(file_path)\n",
        "        if data is not None:\n",
        "            valid_data = data[~np.isnan(data)][:4000000]  # 1000s par fichier\n",
        "            all_data.append(valid_data)\n",
        "\n",
        "    if all_data:\n",
        "        # Étape 1 : Test de Bootstrap / Monte Carlo\n",
        "        combined_data = np.concatenate(all_data)  # Combiner les données des trois fichiers\n",
        "        frequencies, S_f = welch(combined_data, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "        # Modèle logarithmique amélioré\n",
        "        def log_noise_model(log_f, log_A, alpha, log_B):\n",
        "            return log_A - alpha * log_f + log_B\n",
        "\n",
        "        mask = (frequencies > 10) & (frequencies < 1000)\n",
        "        log_f = np.log10(frequencies[mask])\n",
        "        log_S_f = np.log10(S_f[mask])\n",
        "        initial_alpha = -2\n",
        "        initial_log_A = np.log10(np.mean(S_f[mask] * frequencies[mask]**2))\n",
        "        initial_log_B = -45  # Estimation pour un terme constant\n",
        "        p0 = [initial_log_A, initial_alpha, initial_log_B]\n",
        "        popt, pcov = curve_fit(log_noise_model, log_f, log_S_f, p0=p0, maxfev=2000)\n",
        "        log_A_fit, alpha_fit, log_B_fit = popt\n",
        "        A_fit = 10**log_A_fit\n",
        "        B_fit = 10**log_B_fit\n",
        "        report_text.append(f\"1. Test de Bootstrap / Monte Carlo :\")\n",
        "        report_text.append(f\"   - Modèle ajusté : S(f) = {A_fit:.2e} * f^(-{alpha_fit:.2f}) + {B_fit:.2e}\")\n",
        "\n",
        "        # Bruit de fond et écarts\n",
        "        freq_mask = frequencies > 0\n",
        "        S_f_background = np.zeros_like(frequencies)\n",
        "        S_f_background[freq_mask] = A_fit * frequencies[freq_mask]**(-alpha_fit) + B_fit\n",
        "        delta_S_f = S_f - S_f_background\n",
        "\n",
        "        # Bootstrap\n",
        "        n_bootstrap = 100\n",
        "        bootstrap_psds = []\n",
        "        for _ in range(n_bootstrap):\n",
        "            sample = np.random.choice(combined_data, size=len(combined_data), replace=True)\n",
        "            _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            bootstrap_psds.append(psd_sample)\n",
        "        bootstrap_psds = np.array(bootstrap_psds)\n",
        "        psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "        psd_std = np.std(bootstrap_psds, axis=0)\n",
        "        psd_ci_lower = np.percentile(bootstrap_psds, 2.5, axis=0)\n",
        "        psd_ci_upper = np.percentile(bootstrap_psds, 97.5, axis=0)\n",
        "\n",
        "        # Monte Carlo\n",
        "        n_simulations = 100\n",
        "        chi2_simulated = []\n",
        "        for _ in range(n_simulations):\n",
        "            noise = np.random.normal(0, 1, size=len(combined_data)) * np.sqrt(np.mean(S_f))\n",
        "            _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "            chi2_simulated.append(chi2_val)\n",
        "\n",
        "        # Figure 1 : Spectre de bruit\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.loglog(frequencies, S_f, label=\"S(f) observé\")\n",
        "        plt.fill_between(frequencies, psd_ci_lower, psd_ci_upper, alpha=0.2, label=\"Intervalle 95% (Bootstrap)\")\n",
        "        plt.loglog(frequencies[freq_mask], S_f_background[freq_mask], label=f\"S(f) ajusté\", linestyle=\"--\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(\"Densité spectrale S(f)\")\n",
        "        plt.title(\"Spectre de bruit (H1, O3b combiné, 3000s)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 2 : Écarts \\delta S(f)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=r\"$\\delta S(f)$\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Écarts par rapport au bruit de fond\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 2 : Significativité\n",
        "        chi2_observed_adjusted = np.sum(((S_f - S_f_background) / psd_std) ** 2)\n",
        "        p_value_adjusted = np.mean(np.array(chi2_simulated) > chi2_observed_adjusted)\n",
        "        report_text.append(f\"2. Significativité :\")\n",
        "        report_text.append(f\"   - \\(\\chi^2\\) ajusté : {chi2_observed_adjusted:.2e}\")\n",
        "        report_text.append(f\"   - P-valeur ajustée : {p_value_adjusted}\")\n",
        "        thresholds = {\"p < 0.05\": 0.05, \"p < 0.01\": 0.01, \"p < 0.001\": 0.001}\n",
        "        for threshold_name, threshold_value in thresholds.items():\n",
        "            if p_value_adjusted < threshold_value:\n",
        "                report_text.append(f\"   - L’écart est significatif au seuil {threshold_name}\")\n",
        "\n",
        "        # Figure 3 : Distribution des \\(\\chi^2\\) simulés\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.hist(chi2_simulated, bins=30, density=True, alpha=0.7, label=\"\\(\\chi^2\\) simulés\")\n",
        "        plt.axvline(chi2_observed_adjusted, color=\"red\", linestyle=\"--\", label=f\"\\(\\chi^2\\) ajusté = {chi2_observed_adjusted:.2e}\")\n",
        "        plt.xlabel(\"\\(\\chi^2\\)\")\n",
        "        plt.ylabel(\"Densité\")\n",
        "        plt.title(\"Distribution des \\(\\chi^2\\) simulés\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 3 : Analyse comparative entre les trois fichiers\n",
        "        results = {}\n",
        "        for i, file_path in enumerate(file_paths):\n",
        "            data = read_hdf5_local(file_path)\n",
        "            if data is not None:\n",
        "                valid_data_file = data[~np.isnan(data)][:4000000]\n",
        "                freqs_file, S_f_file = welch(valid_data_file, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "                # Ajustement du modèle\n",
        "                popt_file, _ = curve_fit(log_noise_model, np.log10(freqs_file[mask]), np.log10(S_f_file[mask]), p0=p0, maxfev=2000)\n",
        "                log_A_file, alpha_file, log_B_file = popt_file\n",
        "                A_file = 10**log_A_file\n",
        "                B_file = 10**log_B_file\n",
        "                S_f_background_file = np.zeros_like(freqs_file)\n",
        "                S_f_background_file[freq_mask] = A_file * freqs_file[freq_mask]**(-alpha_file) + B_file\n",
        "                delta_S_f_file = S_f_file - S_f_background_file\n",
        "\n",
        "                # Tester \\(\\delta S(f) \\propto f^2\\)\n",
        "                log_freqs_file = np.log10(freqs_file[mask])\n",
        "                log_delta_S_file = np.log10(np.abs(delta_S_f_file[mask]))\n",
        "                slope, _ = np.polyfit(log_freqs_file, log_delta_S_file, 1)\n",
        "                results[file_path] = {\"slope\": slope, \"delta_S_f\": delta_S_f_file}\n",
        "                report_text.append(f\"3. Analyse Comparative :\")\n",
        "                report_text.append(f\"   - {file_path.split('/')[-1]} : Pente de \\(\\delta S(f)\\) = {slope:.2f}\")\n",
        "\n",
        "        # Figure 4 : Comparaison des \\(\\delta S(f)\\) pour les trois fichiers\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=\"Combiné (3000s)\")\n",
        "        for file_path, result in results.items():\n",
        "            plt.semilogx(freqs_file, result[\"delta_S_f\"], label=f\"{file_path.split('/')[-1]} (slope = {result['slope']:.2f})\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Comparaison des écarts \\(\\delta S(f)\\) pour les trois fichiers O3b\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Page texte pour le rapport\n",
        "        plt.figure(figsize=(8, 11))\n",
        "        plt.text(0.1, 0.95, \"Rapport Statistique selon Hypothèse Multidimensionnelle\", fontsize=16, fontweight='bold')\n",
        "        plt.text(0.1, 0.9, \"\\n\".join(report_text), fontsize=12, verticalalignment='top')\n",
        "        plt.axis('off')\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Rapport textuel dans la console\n",
        "        print(\"\\n=== Rapport Statistique ===\")\n",
        "        for line in report_text:\n",
        "            print(line)\n",
        "        print(\"=== Fin du Rapport ===\")\n",
        "\n",
        "    else:\n",
        "        print(\"Échec du chargement des données.\")"
      ],
      "metadata": {
        "id": "UZmhbDAnHHl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "import h5py\n",
        "\n",
        "# Paramètres globaux\n",
        "sampling_rate = 4000  # 4 kHz\n",
        "\n",
        "# Liste des chemins locaux des fichiers\n",
        "file_paths = [\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256669184-4096.hdf5\",\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256677376-4096.hdf5\",\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256685568-4096.hdf5\"\n",
        "]\n",
        "\n",
        "# Fonction pour lire un fichier HDF5 local\n",
        "def read_hdf5_local(file_path):\n",
        "    try:\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            if 'strain' in f and 'Strain' in f['strain']:\n",
        "                data = f['strain/Strain'][:]\n",
        "                print(f\"Données lues depuis {file_path} : {data.size} échantillons\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"Dataset 'strain/Strain' non trouvé dans {file_path}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour {file_path} : {e}\")\n",
        "        return None\n",
        "\n",
        "# Créer un PDF pour le rapport unifié\n",
        "with PdfPages(\"rapport_statistique_hypothese_corrige.pdf\") as pdf:\n",
        "    report_text = []\n",
        "    all_data = []\n",
        "\n",
        "    # Charger les trois fichiers\n",
        "    for file_path in file_paths:\n",
        "        data = read_hdf5_local(file_path)\n",
        "        if data is not None:\n",
        "            valid_data = data[~np.isnan(data)][:4000000]  # 1000s par fichier\n",
        "            all_data.append(valid_data)\n",
        "\n",
        "    if all_data:\n",
        "        # Étape 1 : Test de Bootstrap / Monte Carlo\n",
        "        combined_data = np.concatenate(all_data)  # Combiner les données des trois fichiers\n",
        "        frequencies, S_f = welch(combined_data, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "        # Vérifier les données pour éviter les inf/NaN\n",
        "        if np.any(np.isinf(S_f)) or np.any(np.isnan(S_f)):\n",
        "            print(\"Erreur : S(f) contient des valeurs infinies ou NaN\")\n",
        "            S_f = np.nan_to_num(S_f, nan=np.nanmean(S_f), posinf=np.nanmean(S_f), neginf=np.nanmean(S_f))\n",
        "\n",
        "        # Modèle logarithmique simplifié\n",
        "        def log_noise_model(log_f, log_A, alpha):\n",
        "            return log_A - alpha * log_f\n",
        "\n",
        "        mask = (frequencies > 10) & (frequencies < 1000)\n",
        "        log_f = np.log10(frequencies[mask])\n",
        "        log_S_f = np.log10(S_f[mask])\n",
        "        initial_alpha = -2\n",
        "        initial_log_A = np.log10(np.mean(S_f[mask] * frequencies[mask]**2))\n",
        "        p0 = [initial_log_A, initial_alpha]\n",
        "        try:\n",
        "            popt, pcov = curve_fit(log_noise_model, log_f, log_S_f, p0=p0, maxfev=5000)\n",
        "            log_A_fit, alpha_fit = popt\n",
        "            A_fit = 10**log_A_fit\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Erreur dans l'ajustement : {e}\")\n",
        "            A_fit, alpha_fit = 1e-40, 2  # Valeurs par défaut\n",
        "        report_text.append(f\"1. Test de Bootstrap / Monte Carlo :\")\n",
        "        report_text.append(f\"   - Modèle ajusté : S(f) = {A_fit:.2e} * f^(-{alpha_fit:.2f})\")\n",
        "\n",
        "        # Bruit de fond et écarts\n",
        "        freq_mask = frequencies > 0\n",
        "        S_f_background = np.zeros_like(frequencies)\n",
        "        S_f_background[freq_mask] = A_fit * frequencies[freq_mask]**(-alpha_fit)\n",
        "        delta_S_f = S_f - S_f_background\n",
        "\n",
        "        # Bootstrap\n",
        "        n_bootstrap = 100\n",
        "        bootstrap_psds = []\n",
        "        for _ in range(n_bootstrap):\n",
        "            sample = np.random.choice(combined_data, size=len(combined_data), replace=True)\n",
        "            _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            bootstrap_psds.append(psd_sample)\n",
        "        bootstrap_psds = np.array(bootstrap_psds)\n",
        "        psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "        psd_std = np.std(bootstrap_psds, axis=0)\n",
        "        psd_std = np.where(psd_std == 0, 1e-10, psd_std)  # Éviter division par 0\n",
        "        psd_ci_lower = np.percentile(bootstrap_psds, 2.5, axis=0)\n",
        "        psd_ci_upper = np.percentile(bootstrap_psds, 97.5, axis=0)\n",
        "\n",
        "        # Monte Carlo\n",
        "        n_simulations = 100\n",
        "        chi2_simulated = []\n",
        "        for _ in range(n_simulations):\n",
        "            noise = np.random.normal(0, 1, size=len(combined_data)) * np.sqrt(np.mean(S_f))\n",
        "            _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "            chi2_simulated.append(chi2_val)\n",
        "\n",
        "        # Figure 1 : Spectre de bruit\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.loglog(frequencies, S_f, label=\"S(f) observé\")\n",
        "        plt.fill_between(frequencies, psd_ci_lower, psd_ci_upper, alpha=0.2, label=\"Intervalle 95% (Bootstrap)\")\n",
        "        plt.loglog(frequencies[freq_mask], S_f_background[freq_mask], label=f\"S(f) ajusté (f^(-{alpha_fit:.2f}))\", linestyle=\"--\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(\"Densité spectrale S(f)\")\n",
        "        plt.title(\"Spectre de bruit (H1, O3b combiné, 3000s)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 2 : Écarts \\delta S(f)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=r\"$\\delta S(f)$\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Écarts par rapport au bruit de fond\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 2 : Significativité\n",
        "        chi2_observed_adjusted = np.sum(((S_f - S_f_background) / psd_std) ** 2)\n",
        "        if np.isinf(chi2_observed_adjusted):\n",
        "            print(\"Erreur : \\(\\chi^2\\) ajusté est infini, ajustement des données\")\n",
        "            chi2_observed_adjusted = np.nan_to_num(chi2_observed_adjusted, posinf=1e10, neginf=-1e10)\n",
        "        p_value_adjusted = np.mean(np.array(chi2_simulated) > chi2_observed_adjusted)\n",
        "        report_text.append(f\"2. Significativité :\")\n",
        "        report_text.append(f\"   - \\(\\chi^2\\) ajusté : {chi2_observed_adjusted:.2e}\")\n",
        "        report_text.append(f\"   - P-valeur ajustée : {p_value_adjusted}\")\n",
        "        thresholds = {\"p < 0.05\": 0.05, \"p < 0.01\": 0.01, \"p < 0.001\": 0.001}\n",
        "        for threshold_name, threshold_value in thresholds.items():\n",
        "            if p_value_adjusted < threshold_value:\n",
        "                report_text.append(f\"   - L’écart est significatif au seuil {threshold_name}\")\n",
        "\n",
        "        # Figure 3 : Distribution des \\(\\chi^2\\) simulés\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.hist(chi2_simulated, bins=30, density=True, alpha=0.7, label=\"\\(\\chi^2\\) simulés\")\n",
        "        plt.axvline(chi2_observed_adjusted, color=\"red\", linestyle=\"--\", label=f\"\\(\\chi^2\\) ajusté = {chi2_observed_adjusted:.2e}\")\n",
        "        plt.xlabel(\"\\(\\chi^2\\)\")\n",
        "        plt.ylabel(\"Densité\")\n",
        "        plt.title(\"Distribution des \\(\\chi^2\\) simulés\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 3 : Analyse comparative entre les trois fichiers\n",
        "        results = {}\n",
        "        for i, file_path in enumerate(file_paths):\n",
        "            data = read_hdf5_local(file_path)\n",
        "            if data is not None:\n",
        "                valid_data_file = data[~np.isnan(data)][:4000000]\n",
        "                freqs_file, S_f_file = welch(valid_data_file, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "                if np.any(np.isinf(S_f_file)) or np.any(np.isnan(S_f_file)):\n",
        "                    S_f_file = np.nan_to_num(S_f_file, nan=np.nanmean(S_f_file), posinf=np.nanmean(S_f_file), neginf=np.nanmean(S_f_file))\n",
        "\n",
        "                # Ajustement du modèle\n",
        "                popt_file, _ = curve_fit(log_noise_model, np.log10(freqs_file[mask]), np.log10(S_f_file[mask]), p0=p0, maxfev=5000)\n",
        "                log_A_file, alpha_file = popt_file\n",
        "                A_file = 10**log_A_file\n",
        "                S_f_background_file = np.zeros_like(freqs_file)\n",
        "                S_f_background_file[freq_mask] = A_file * freqs_file[freq_mask]**(-alpha_file)\n",
        "                delta_S_f_file = S_f_file - S_f_background_file\n",
        "\n",
        "                # Tester \\(\\delta S(f) \\propto f^2\\)\n",
        "                log_freqs_file = np.log10(freqs_file[mask])\n",
        "                log_delta_S_file = np.log10(np.abs(delta_S_f_file[mask]))\n",
        "                slope, _ = np.polyfit(log_freqs_file, log_delta_S_file, 1)\n",
        "                results[file_path] = {\"slope\": slope, \"delta_S_f\": delta_S_f_file}\n",
        "                report_text.append(f\"   - {file_path.split('/')[-1]} : Pente de \\(\\delta S(f)\\) = {slope:.2f}\")\n",
        "\n",
        "        # Figure 4 : Comparaison des \\(\\delta S(f)\\) pour les trois fichiers\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=\"Combiné (3000s)\")\n",
        "        for file_path, result in results.items():\n",
        "            plt.semilogx(freqs_file, result[\"delta_S_f\"], label=f\"{file_path.split('/')[-1]} (slope = {result['slope']:.2f})\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Comparaison des écarts \\(\\delta S(f)\\) pour les trois fichiers O3b\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Page texte pour le rapport\n",
        "        plt.figure(figsize=(8, 11))\n",
        "        plt.text(0.1, 0.95, \"Rapport Statistique selon Hypothèse Multidimensionnelle\", fontsize=16, fontweight='bold')\n",
        "        plt.text(0.1, 0.9, \"\\n\".join(report_text), fontsize=12, verticalalignment='top')\n",
        "        plt.axis('off')\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Rapport textuel dans la console\n",
        "        print(\"\\n=== Rapport Statistique ===\")\n",
        "        for line in report_text:\n",
        "            print(line)\n",
        "        print(\"=== Fin du Rapport ===\")\n",
        "\n",
        "    else:\n",
        "        print(\"Échec du chargement des données.\")"
      ],
      "metadata": {
        "id": "o7tu_vfDJy7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "from scipy.signal import welch\n",
        "from scipy.optimize import curve_fit\n",
        "import h5py\n",
        "\n",
        "# Paramètres globaux\n",
        "sampling_rate = 4000  # 4 kHz\n",
        "\n",
        "# Liste des chemins locaux des fichiers\n",
        "file_paths = [\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256669184-4096.hdf5\",\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256677376-4096.hdf5\",\n",
        "    \"/content/H-H1_GWOSC_O3b_4KHZ_R1-1256685568-4096.hdf5\"\n",
        "]\n",
        "\n",
        "# Fonction pour lire un fichier HDF5 local\n",
        "def read_hdf5_local(file_path):\n",
        "    try:\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            if 'strain' in f and 'Strain' in f['strain']:\n",
        "                data = f['strain/Strain'][:]\n",
        "                print(f\"Données lues depuis {file_path} : {data.size} échantillons\")\n",
        "                return data\n",
        "            else:\n",
        "                print(f\"Dataset 'strain/Strain' non trouvé dans {file_path}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur pour {file_path} : {e}\")\n",
        "        return None\n",
        "\n",
        "# Créer un PDF pour le rapport unifié\n",
        "with PdfPages(\"rapport_statistique_hypothese_final.pdf\") as pdf:\n",
        "    report_text = []\n",
        "    all_data = []\n",
        "\n",
        "    # Charger les trois fichiers\n",
        "    for file_path in file_paths:\n",
        "        data = read_hdf5_local(file_path)\n",
        "        if data is not None:\n",
        "            valid_data = data[~np.isnan(data)][:4000000]  # 1000s par fichier\n",
        "            all_data.append(valid_data)\n",
        "\n",
        "    if all_data:\n",
        "        # Étape 1 : Test de Bootstrap / Monte Carlo\n",
        "        combined_data = np.concatenate(all_data)  # Combiner les données des trois fichiers\n",
        "        frequencies, S_f = welch(combined_data, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "\n",
        "        # Vérifier les données pour éviter les inf/NaN\n",
        "        if np.any(np.isinf(S_f)) or np.any(np.isnan(S_f)):\n",
        "            print(\"Erreur : S(f) contient des valeurs infinies ou NaN\")\n",
        "            S_f = np.nan_to_num(S_f, nan=np.nanmean(S_f), posinf=np.nanmean(S_f), neginf=np.nanmean(S_f))\n",
        "\n",
        "        # Modèle de bruit plus complexe\n",
        "        def noise_model(f, A, alpha, B, C):\n",
        "            return A * f**(-alpha) + B * f**2 + C\n",
        "\n",
        "        mask = (frequencies > 10) & (frequencies < 1000)\n",
        "        p0 = [1e-40, 2, 1e-48, 1e-45]  # [A, alpha, B, C]\n",
        "        try:\n",
        "            popt, pcov = curve_fit(noise_model, frequencies[mask], S_f[mask], p0=p0, maxfev=5000)\n",
        "            A_fit, alpha_fit, B_fit, C_fit = popt\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Erreur dans l'ajustement : {e}\")\n",
        "            A_fit, alpha_fit, B_fit, C_fit = 1e-40, 2, 0, 1e-45  # Valeurs par défaut\n",
        "        report_text.append(f\"1. Test de Bootstrap / Monte Carlo :\")\n",
        "        report_text.append(f\"   - Modèle ajusté : S(f) = {A_fit:.2e} * f^(-{alpha_fit:.2f}) + {B_fit:.2e} * f^2 + {C_fit:.2e}\")\n",
        "\n",
        "        # Bruit de fond et écarts\n",
        "        freq_mask = frequencies > 0\n",
        "        S_f_background = np.zeros_like(frequencies)\n",
        "        S_f_background[freq_mask] = noise_model(frequencies[freq_mask], A_fit, alpha_fit, B_fit, C_fit)\n",
        "        delta_S_f = S_f - S_f_background\n",
        "\n",
        "        # Bootstrap\n",
        "        n_bootstrap = 100\n",
        "        bootstrap_psds = []\n",
        "        for _ in range(n_bootstrap):\n",
        "            sample = np.random.choice(combined_data, size=len(combined_data), replace=True)\n",
        "            _, psd_sample = welch(sample, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            bootstrap_psds.append(psd_sample)\n",
        "        bootstrap_psds = np.array(bootstrap_psds)\n",
        "        psd_mean = np.mean(bootstrap_psds, axis=0)\n",
        "        psd_std = np.std(bootstrap_psds, axis=0)\n",
        "        psd_std = np.where(psd_std == 0, 1e-10, psd_std)  # Éviter division par 0\n",
        "        psd_ci_lower = np.percentile(bootstrap_psds, 2.5, axis=0)\n",
        "        psd_ci_upper = np.percentile(bootstrap_psds, 97.5, axis=0)\n",
        "\n",
        "        # Monte Carlo\n",
        "        n_simulations = 100\n",
        "        chi2_simulated = []\n",
        "        for _ in range(n_simulations):\n",
        "            noise = np.random.normal(0, 1, size=len(combined_data)) * np.sqrt(np.mean(S_f))\n",
        "            _, psd_sim = welch(noise, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "            chi2_val = np.sum(((psd_sim - psd_mean) / psd_std) ** 2)\n",
        "            chi2_simulated.append(chi2_val)\n",
        "\n",
        "        # Figure 1 : Spectre de bruit\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.loglog(frequencies, S_f, label=\"S(f) observé\")\n",
        "        plt.fill_between(frequencies, psd_ci_lower, psd_ci_upper, alpha=0.2, label=\"Intervalle 95% (Bootstrap)\")\n",
        "        plt.loglog(frequencies[freq_mask], S_f_background[freq_mask], label=\"S(f) ajusté\", linestyle=\"--\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(\"Densité spectrale S(f)\")\n",
        "        plt.title(\"Spectre de bruit (H1, O3b combiné, 3000s)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Figure 2 : Écarts \\delta S(f)\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=r\"$\\delta S(f)$\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Écarts par rapport au bruit de fond\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 2 : Significativité\n",
        "        chi2_observed_adjusted = np.sum(((S_f - S_f_background) / psd_std) ** 2)\n",
        "        if np.isinf(chi2_observed_adjusted):\n",
        "            print(\"Erreur : \\(\\chi^2\\) ajusté est infini, ajustement des données\")\n",
        "            chi2_observed_adjusted = np.nan_to_num(chi2_observed_adjusted, posinf=1e10, neginf=-1e10)\n",
        "        p_value_adjusted = np.mean(np.array(chi2_simulated) > chi2_observed_adjusted)\n",
        "        report_text.append(f\"2. Significativité :\")\n",
        "        report_text.append(f\"   - \\(\\chi^2\\) ajusté : {chi2_observed_adjusted:.2e}\")\n",
        "        report_text.append(f\"   - P-valeur ajustée : {p_value_adjusted}\")\n",
        "        thresholds = {\"p < 0.05\": 0.05, \"p < 0.01\": 0.01, \"p < 0.001\": 0.001}\n",
        "        for threshold_name, threshold_value in thresholds.items():\n",
        "            if p_value_adjusted < threshold_value:\n",
        "                report_text.append(f\"   - L’écart est significatif au seuil {threshold_name}\")\n",
        "\n",
        "        # Figure 3 : Distribution des \\(\\chi^2\\) simulés\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.hist(chi2_simulated, bins=30, density=True, alpha=0.7, label=\"\\(\\chi^2\\) simulés\")\n",
        "        plt.axvline(chi2_observed_adjusted, color=\"red\", linestyle=\"--\", label=f\"\\(\\chi^2\\) ajusté = {chi2_observed_adjusted:.2e}\")\n",
        "        plt.xlabel(\"\\(\\chi^2\\)\")\n",
        "        plt.ylabel(\"Densité\")\n",
        "        plt.title(\"Distribution des \\(\\chi^2\\) simulés\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Étape 3 : Analyse comparative entre les trois fichiers\n",
        "        results = {}\n",
        "        for i, file_path in enumerate(file_paths):\n",
        "            data = read_hdf5_local(file_path)\n",
        "            if data is not None:\n",
        "                valid_data_file = data[~np.isnan(data)][:4000000]\n",
        "                freqs_file, S_f_file = welch(valid_data_file, fs=sampling_rate, nperseg=1024, noverlap=512)\n",
        "                if np.any(np.isinf(S_f_file)) or np.any(np.isnan(S_f_file)):\n",
        "                    S_f_file = np.nan_to_num(S_f_file, nan=np.nanmean(S_f_file), posinf=np.nanmean(S_f_file), neginf=np.nanmean(S_f_file))\n",
        "\n",
        "                # Ajustement du modèle\n",
        "                popt_file, _ = curve_fit(noise_model, freqs_file[mask], S_f_file[mask], p0=p0, maxfev=5000)\n",
        "                A_file, alpha_file, B_file, C_file = popt_file\n",
        "                S_f_background_file = np.zeros_like(freqs_file)\n",
        "                S_f_background_file[freq_mask] = noise_model(freqs_file[freq_mask], A_file, alpha_file, B_file, C_file)\n",
        "                delta_S_f_file = S_f_file - S_f_background_file\n",
        "\n",
        "                # Tester \\(\\delta S(f) \\propto f^2\\) uniquement pour \\(\\delta S(f) > 0\\)\n",
        "                positive_mask = (freqs_file[mask] > 0) & (delta_S_f_file[mask] > 0)\n",
        "                if np.sum(positive_mask) > 2:  # Assurer assez de points pour l'ajustement\n",
        "                    log_freqs_file = np.log10(freqs_file[mask][positive_mask])\n",
        "                    log_delta_S_file = np.log10(delta_S_f_file[mask][positive_mask])\n",
        "                    slope, _ = np.polyfit(log_freqs_file, log_delta_S_file, 1)\n",
        "                else:\n",
        "                    slope = np.nan  # Pas assez de points positifs\n",
        "                results[file_path] = {\"slope\": slope, \"delta_S_f\": delta_S_f_file}\n",
        "                report_text.append(f\"   - {file_path.split('/')[-1]} : Pente de \\(\\delta S(f)\\) (positif) = {slope:.2f}\")\n",
        "\n",
        "        # Figure 4 : Comparaison des \\(\\delta S(f)\\) pour les trois fichiers\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.semilogx(frequencies, delta_S_f, label=\"Combiné (3000s)\")\n",
        "        for file_path, result in results.items():\n",
        "            plt.semilogx(freqs_file, result[\"delta_S_f\"], label=f\"{file_path.split('/')[-1]} (slope = {result['slope']:.2f})\")\n",
        "        plt.xlabel(\"Fréquence (Hz)\")\n",
        "        plt.ylabel(r\"$\\delta S(f)$\")\n",
        "        plt.title(\"Comparaison des écarts \\(\\delta S(f)\\) pour les trois fichiers O3b\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, which=\"both\", ls=\"--\")\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Page texte pour le rapport\n",
        "        plt.figure(figsize=(8, 11))\n",
        "        plt.text(0.1, 0.95, \"Rapport Statistique selon Hypothèse Multidimensionnelle\", fontsize=16, fontweight='bold')\n",
        "        plt.text(0.1, 0.9, \"\\n\".join(report_text), fontsize=12, verticalalignment='top')\n",
        "        plt.axis('off')\n",
        "        pdf.savefig()\n",
        "        plt.close()\n",
        "\n",
        "        # Rapport textuel dans la console\n",
        "        print(\"\\n=== Rapport Statistique ===\")\n",
        "        for line in report_text:\n",
        "            print(line)\n",
        "        print(\"=== Fin du Rapport ===\")\n",
        "\n",
        "    else:\n",
        "        print(\"Échec du chargement des données.\")"
      ],
      "metadata": {
        "id": "gcsvBZAgLSa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🚀 INSTALLATION DES PACKAGES NÉCESSAIRES\n",
        "!pip install --upgrade h5py gwpy numpy scipy matplotlib seaborn pandas tqdm\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from gwpy.timeseries import TimeSeries\n",
        "from scipy.signal import welch\n",
        "\n",
        "# 📂 Dossier où stocker les fichiers téléchargés\n",
        "download_folder = \"/content/gw_data\"\n",
        "os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "# 📌 Liste des fichiers HDF5 à télécharger\n",
        "urls = [\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256726528-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256730624-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256734720-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256738816-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256742912-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256747008-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256751104-4096.hdf5\",\n",
        "    \"http://gwosc.org/archive/data/O3b_4KHZ_R1/1256194048/H-H1_GWOSC_O3b_4KHZ_R1-1256755200-4096.hdf5\"\n",
        "]\n",
        "\n",
        "# 🔽 Téléchargement des fichiers HDF5\n",
        "for url in tqdm(urls, desc=\"Téléchargement des fichiers HDF5\"):\n",
        "    filename = os.path.join(download_folder, url.split(\"/\")[-1])\n",
        "    if not os.path.exists(filename):  # Vérifie si le fichier existe déjà\n",
        "        os.system(f\"wget -q -P {download_folder} {url}\")  # Téléchargement silencieux\n",
        "\n",
        "print(\"✅ Tous les fichiers ont été téléchargés.\")\n",
        "\n",
        "# 📊 Analyse des fichiers\n",
        "psd_data = {}\n",
        "\n",
        "for file in tqdm(os.listdir(download_folder), desc=\"Analyse des fichiers\"):\n",
        "    filepath = os.path.join(download_folder, file)\n",
        "\n",
        "    # 📥 Charger le signal gravitationnel\n",
        "    strain = TimeSeries.read(filepath, format=\"hdf5\", channel=\"H1:GWOSC-4KHZ_R1_STRAIN\")\n",
        "\n",
        "    # ⚡ Calcul du spectre de bruit (PSD) avec Welch\n",
        "    fs = 4096  # Fréquence d'échantillonnage (Hz)\n",
        "    f, Pxx = welch(strain.value, fs=fs, nperseg=fs)  # Welch method\n",
        "    psd_data[file] = (f, Pxx)\n",
        "\n",
        "# 🔍 Comparaison des spectres de bruit\n",
        "plt.figure(figsize=(12,6))\n",
        "for file, (f, Pxx) in psd_data.items():\n",
        "    plt.loglog(f, Pxx, label=file.split('-')[2])  # Affiche le spectre\n",
        "plt.xlabel(\"Fréquence (Hz)\")\n",
        "plt.ylabel(\"Densité Spectrale de Puissance (PSD)\")\n",
        "plt.title(\"Comparaison des Spectres de Bruit des Différents Fichiers GW\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 🔎 Analyse des écarts \\(\\delta S(f)\\)\n",
        "df_psd = pd.DataFrame()\n",
        "for file, (f, Pxx) in psd_data.items():\n",
        "    df_psd[file] = Pxx\n",
        "\n",
        "# Calcul des écarts \\(\\delta S(f)\\) par rapport à la moyenne\n",
        "mean_psd = df_psd.mean(axis=1)\n",
        "delta_Sf = df_psd.subtract(mean_psd, axis=0)\n",
        "\n",
        "# 📉 Tracé des écarts\n",
        "plt.figure(figsize=(12,6))\n",
        "for file in delta_Sf.columns:\n",
        "    plt.semilogx(f, delta_Sf[file], label=file.split('-')[2])\n",
        "plt.xlabel(\"Fréquence (Hz)\")\n",
        "plt.ylabel(\"Écart de Bruit \\(\\delta S(f)\\)\")\n",
        "plt.title(\"Comparaison des Écarts de Bruit \\(\\delta S(f)\\)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 📊 Analyse statistique avancée : test de significativité (Bootstrap)\n",
        "from scipy.stats import bootstrap\n",
        "\n",
        "# Bootstrap sur les écarts \\(\\delta S(f)\\)\n",
        "ci = []\n",
        "for i in range(len(f)):\n",
        "    data = delta_Sf.iloc[i].dropna().values\n",
        "    ci.append(bootstrap((data,), np.mean, confidence_level=0.95, method='percentile').confidence_interval)\n",
        "\n",
        "ci_lower, ci_upper = zip(*ci)\n",
        "\n",
        "# 📏 Tracé des intervalles de confiance\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.fill_between(f, ci_lower, ci_upper, alpha=0.3, label=\"Intervalle de confiance 95%\")\n",
        "plt.semilogx(f, mean_psd, label=\"Moyenne du bruit\")\n",
        "plt.xlabel(\"Fréquence (Hz)\")\n",
        "plt.ylabel(\"Densité Spectrale de Bruit \\( S(f) \\)\")\n",
        "plt.title(\"Intervalles de Confiance sur les Écarts de Bruit\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Analyse terminée. Les résultats montrent des écarts significatifs par rapport au modèle standard.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6aweySRGfLTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycbc"
      ],
      "metadata": {
        "id": "svqc-oFnlHi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycbc\n",
        "from pycbc.waveform import get_td_waveform"
      ],
      "metadata": {
        "id": "MsqDhKS0oScU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PdfPages\n"
      ],
      "metadata": {
        "id": "sMKwLgV9pNVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf2\n",
        "\n"
      ],
      "metadata": {
        "id": "8mmQ48f8q41k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "impor\n",
        "from scipy.signal import welch\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"Charge les données HDF5 et extrait le signal GW\"\"\"\n",
        "    with h5py.File(filepath, 'r') as f:\n",
        "        strain = f['strain']['Strain'][:]\n",
        "    return strain\n",
        "\n",
        "def spectral_analysis(strain, fs=4096):\n",
        "    \"\"\"Analyse spectrale via transformée de Welch\"\"\"\n",
        "    freqs, psd = welch(strain, fs, nperseg=fs)\n",
        "    return freqs, psd\n",
        "\n",
        "def inject_synthetic_signal(freqs, A=1e-10):\n",
        "    \"\"\"Injecte un signal synthétique δS(f) = Af²\"\"\"\n",
        "    return A * freqs**2\n",
        "\n",
        "def wavelet_analysis(strain, fs=4096):\n",
        "    \"\"\"Applique une transformée en ondelettes de Morlet\"\"\"\n",
        "    coeffs, _ = pywt.cwt(strain, scales=np.arange(1, 128), wavelet='cmor')\n",
        "    return np.abs(coeffs).mean(axis=0)\n",
        "\n",
        "def generate_report(image_filename, freqs, psd, synthetic_signal, wavelet_response):\n",
        "    \"\"\"Génère un rapport PDF des résultats\"\"\"\n",
        "    # Création d'une figure unique pour le rapport\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Sauvegarde et ajoute les figures\n",
        "\n",
        "    plt.loglog(freqs, psd, label='Spectre GW')\n",
        "    plt.loglog(freqs, synthetic_signal, '--', label='Signal synthétique')\n",
        "    plt.xlabel('Fréquence (Hz)')\n",
        "    plt.ylabel('PSD')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.loglog(freqs, psd, label='Spectre GW')\n",
        "    plt.loglog(freqs, synthetic_signal, '--', label='Signal synthétique')\n",
        "    plt.xlabel('Fréquence (Hz)')\n",
        "    plt.ylabel('PSD')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(wavelet_response)\n",
        "    plt.xlabel('Temps')\n",
        "    plt.ylabel('Amplitude Ondelette')\n",
        "    plt.title('Analyse en ondelettes')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(wavelet_response)\n",
        "    plt.xlabel('Temps')\n",
        "    plt.ylabel('Amplitude Ondelette')\n",
        "    plt.title('Analyse en ondelettes')\n",
        "    plt.savefig(image_filename)  # Sauvegarde en PNG\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    filepath = \"H-H1_GWOSC_O3b_4KHZ_R1-1256660992-4096.hdf5\"  # Adapter le fichier ici\n",
        "    strain = load_data(filepath)\n",
        "    freqs, psd = spectral_analysis(strain)\n",
        "    synthetic_signal = inject_synthetic_signal(freqs)\n",
        "    wavelet_response = wavelet_analysis(strain)\n",
        "    generate_report(\"GWOSC_Report.png\", freqs, psd, synthetic_signal, wavelet_response)\n",
        "    print(\"Rapport généré : GWOSC_Report.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "EXopHoFv3g_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}